{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8966cdbf-182e-4132-b70b-569647e42da5",
   "metadata": {},
   "source": [
    "# ğŸ§¬ Bacterial Cell Lineage Tracker (TrackMate â†’ Statistics)\n",
    "\n",
    "**Pipeline Step 1:** Convert TrackMate tracking data into cell-by-cell statistics with lineage information\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ What This Notebook Does\n",
    "\n",
    "This pipeline analyzes bacterial cell division from **TrackMate** (Fiji/ImageJ) tracking exports:\n",
    "\n",
    "1. **Loads** TrackMate `spots.csv` and `edges.csv` files\n",
    "2. **Reconstructs** mother-daughter lineage trees using graph theory\n",
    "3. **Calculates** growth rates, division timing, and morphology metrics (length, width, area, etc.)\n",
    "4. **Validates** division events using physics-based quality filters\n",
    "5. **Exports** clean statistics ready for population-level comparison\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¥ Required Input Files\n",
    "\n",
    "From TrackMate (File â†’ Export â†’ Export to CSV):\n",
    "- `*_spots.csv` - Cell positions, sizes, and shape metrics over time\n",
    "- `*_edges.csv` - Parent-child linkages between timepoints\n",
    "\n",
    "**Important:** Files must be paired (same prefix). Example: 20260101_L206K_mreB_s1_spots.csv / 20260101_L206K_mreB_s1_edges.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cf429-98d9-4cb2-b174-c543b746318e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¤ Output Files (Per Input Pair)\n",
    "\n",
    "- `*_AllBranchesStats.csv` - Cell-by-cell statistics (main output)\n",
    "- `*_LineagePaths.csv` - Complete lineage paths (t1â†’t2â†’t3â†’...)\n",
    "- `*_CVReport.txt` - Coefficient of variation diagnostics\n",
    "- `*_LineageTrees.png` - Visual gallery of example lineage trees\n",
    "- `*_Network_*.png` - Graph diagram of branching structure\n",
    "- `LineagePathDetails/*.png` - Individual high-quality lineage traces\n",
    "\n",
    "**Plus combined outputs:**\n",
    "- `AllBranchesStats.csv` - All experiments merged\n",
    "- `AllLineagePaths.csv` - All lineages merged\n",
    "- `CombinedCVReport.txt` - Population-level variance report\n",
    "- `GrowthModelProof.txt` - Adder/Sizer/Timer classification per file\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "\n",
    "1. **Run Cell 2** to install dependencies\n",
    "2. **Edit Cell 3** to set your imaging parameters (frame interval, thresholds)\n",
    "3. **Run Cell 4** to select your input folder\n",
    "4. **Run All** remaining cells\n",
    "\n",
    "**Estimated runtime:** ~30 seconds per input file pair\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Need Help?\n",
    "\n",
    "- **TrackMate export not working?** See [docs/trackmate_export_guide.md](../docs/trackmate_export_guide.md)\n",
    "- **Getting validation errors?** Adjust quality thresholds in Cell 3\n",
    "- **Want to understand the statistics?** See [docs/output_column_guide.md](../docs/output_column_guide.md)\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/github/JoeVPhD/TrackMate_CellPose_Bacterial_Generation_Determination/blob/main/TrackMate_CellPose_Bacterial_Generation_Determination.ipynb#scrollTo=ad539942-af8e-44bb-b77b-b9a1ad7e1dce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659a15fb-78b4-4442-afc8-26d77c567c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸  Environment: Jupyter Notebook\n",
      "âœ… All imports successful!\n",
      "ğŸ“Š Pandas: 2.2.2 | NumPy: 1.26.4 | NetworkX: 3.3\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ INSTALLATION & IMPORTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_JUPYTER = 'ipykernel' in sys.modules\n",
    "\n",
    "print(f\"ğŸ–¥ï¸  Environment: {'Google Colab' if IN_COLAB else 'Jupyter Notebook' if IN_JUPYTER else 'Python Script'}\")\n",
    "\n",
    "# Install dependencies (Colab only)\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Installing required packages...\")\n",
    "    !pip install -q pandas numpy matplotlib seaborn scipy networkx\n",
    "    from google.colab import drive # Import drive here to be ready\n",
    "    print(\"âœ… Installation complete!\")\n",
    "\n",
    "# Core imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.stats import linregress, skew, kurtosis\n",
    "\n",
    "# Configure matplotlib for notebook display\n",
    "if IN_COLAB or IN_JUPYTER:\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    matplotlib.use('Agg')  # File output only\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"ğŸ“Š Pandas: {pd.__version__} | NumPy: {np.__version__} | NetworkX: {nx.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c11a88e-2c23-4386-9684-bdb5ca688c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded!\n",
      "â±ï¸  Frame interval: 2.0 minutes\n",
      "ğŸ¯ Quality mode: Standard\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âš™ï¸ ANALYSIS PARAMETERS (EDIT THESE!)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“¸ IMAGING PARAMETERS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FRAME_INTERVAL = 2.0  # Minutes per frame (CRITICAL: match your microscope settings!)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§ª QUALITY FILTERS (Relaxed = More Data, Strict = Cleaner Data)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Division Symmetry\n",
    "THRESH_BALANCE = 0.75       # Sister size ratio (0.75 = 25% asymmetry allowed)\n",
    "                            # Smaller daughter / Larger daughter must be â‰¥ this\n",
    "\n",
    "# Mass Conservation\n",
    "THRESH_CONS_MIN = 0.85      # Min mass conservation (daughters vs mother area)\n",
    "THRESH_CONS_MAX = 1.15      # Max mass conservation\n",
    "\n",
    "# Spatial Jumps\n",
    "THRESH_JUMP_MULT = 1.5      # Max jump = cell_length Ã— this multiplier\n",
    "                            # Prevents tracking errors (teleporting cells)\n",
    "\n",
    "# Growth Rate\n",
    "THRESH_GROWTH = 0.5         # Max instantaneous growth rate (50% per frame)\n",
    "                            # Flags measurement artifacts\n",
    "\n",
    "# Exponential Fit Quality\n",
    "THRESH_R2 = 0.90            # Minimum RÂ² for growth curve (0.90 = 90% fit)\n",
    "                            # Only accept clean exponential growth\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“ MINIMUM TRACK LENGTHS (Frames)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MIN_DURATION_MOTHER = 5     # Gen 1 must last â‰¥5 frames (~10 min @ 2min/frame)\n",
    "MIN_DURATION_DAUGHTER = 3   # Gen 2+ can be shorter (allow faster dividers)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“Š VISUALIZATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MAX_LINEAGE_EXAMPLES = 6    # Number of example trees to plot\n",
    "MAX_DETAIL_PLOTS = 20       # Number of individual lineage traces\n",
    "\n",
    "print(\"âœ… Configuration loaded!\")\n",
    "print(f\"â±ï¸  Frame interval: {FRAME_INTERVAL} minutes\")\n",
    "print(f\"ğŸ¯ Quality mode: {'Strict' if THRESH_R2 >= 0.95 else 'Standard' if THRESH_R2 >= 0.85 else 'Relaxed'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d55b3b-3282-47b7-b742-01b04e707810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Looking for TrackMate export files...\n",
      "\n",
      "âœ… Found 4 valid TrackMate file pairs in:\n",
      "   ğŸ“ D:\\Microscopy\\260116\\Omnipose Trackmate Output\n",
      "\n",
      "ğŸ“„ First 5 files:\n",
      "   1. 260116_A11_s5_spots.csv\n",
      "   2. 260116_A11_spoT_s6_spots.csv\n",
      "   3. 260116_A11_spoT_s7_spots.csv\n",
      "   4. 260116_A11_spoT_s8_spots.csv\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“‚ INPUT FOLDER SELECTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def _browse_dirs_cli(start_dir: Path) -> Path:\n",
    "    \"\"\"Simple interactive directory browser (works in Colab).\"\"\"\n",
    "    cur = start_dir\n",
    "\n",
    "    while True:\n",
    "        if not cur.exists():\n",
    "            print(f\"âŒ Folder not found: {cur}\")\n",
    "            cur = start_dir\n",
    "            continue\n",
    "\n",
    "        # Get subdirectories only\n",
    "        try:\n",
    "            subdirs = sorted([p for p in cur.iterdir() if p.is_dir()],\n",
    "                             key=lambda p: p.name.lower())\n",
    "        except PermissionError:\n",
    "            print(\"ğŸš« Permission denied. Going up...\")\n",
    "            cur = cur.parent\n",
    "            continue\n",
    "\n",
    "        print(\"\\n\" + \"â”€\" * 70)\n",
    "        print(f\"ğŸ“ Current folder: {cur}\")\n",
    "        print(\"Type:\")\n",
    "        print(\"  0  = âœ… SELECT THIS FOLDER\")\n",
    "        print(\"  .. = â¬†ï¸ Go up one level\")\n",
    "        print(\"\\nSubfolders:\")\n",
    "        for i, d in enumerate(subdirs, start=1):\n",
    "            print(f\"  {i:>2}. {d.name}/\")\n",
    "\n",
    "        choice = input(\"\\nğŸ‘‡ Select number (or '0' to finish): \").strip()\n",
    "\n",
    "        if choice == \"0\":\n",
    "            return cur\n",
    "\n",
    "        if choice == \"..\":\n",
    "            cur = cur.parent\n",
    "            continue\n",
    "\n",
    "        if choice.isdigit():\n",
    "            idx = int(choice)\n",
    "            if 1 <= idx <= len(subdirs):\n",
    "                cur = subdirs[idx - 1]\n",
    "                continue\n",
    "        \n",
    "        # Handle manual path pasting or errors\n",
    "        p = Path(choice)\n",
    "        if p.is_absolute() and p.exists() and p.is_dir():\n",
    "            cur = p\n",
    "        else:\n",
    "            print(\"âŒ Invalid selection. Enter a number, '..', '0', or a valid absolute path.\")\n",
    "\n",
    "def select_input_folder():\n",
    "    \"\"\"Platform-aware folder picker.\"\"\"\n",
    "    if IN_COLAB:\n",
    "        print(\"ğŸ”— Google Colab detected!\")\n",
    "        \n",
    "        # 1. Mount Drive if missing\n",
    "        if not Path(\"/content/drive\").exists():\n",
    "            print(\"ğŸ“Œ Mounting Google Drive...\")\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "        \n",
    "        # 2. Start browser at MyDrive\n",
    "        print(\"\\nğŸš€ Launching Folder Browser...\")\n",
    "        return _browse_dirs_cli(Path(\"/content/drive/MyDrive\"))\n",
    "\n",
    "    else:\n",
    "        # Local Jupyter: use system dialog\n",
    "        import tkinter as tk\n",
    "        from tkinter import filedialog\n",
    "        \n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)\n",
    "        \n",
    "        print(\"Please check your taskbar for the folder selection window...\")\n",
    "        folder = filedialog.askdirectory(title=\"Select folder containing TrackMate CSVs\")\n",
    "        root.destroy()\n",
    "        \n",
    "        return Path(folder) if folder else None\n",
    "\n",
    "# --- EXECUTE ---\n",
    "INPUT_FOLDER = select_input_folder()\n",
    "\n",
    "if INPUT_FOLDER:\n",
    "    print(f\"\\nâœ… Selected: {INPUT_FOLDER}\")\n",
    "    # Verify contents\n",
    "    spots = list(INPUT_FOLDER.glob(\"*spots.csv\"))\n",
    "    print(f\"Found {len(spots)} spot files.\")\n",
    "else:\n",
    "    print(\"\\nâŒ No folder selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507a8f5e-e6b5-4971-b706-d46f51991281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results will be saved to:\n",
      "   ğŸ“ D:\\Microscopy\\260116\\Omnipose Trackmate Output\\PathResults\n",
      "\n",
      "ğŸ“Š Output structure:\n",
      "   PathResults/\n",
      "   â”œâ”€â”€ AllBranchesStats.csv           (â­ Main output)\n",
      "   â”œâ”€â”€ AllLineagePaths.csv\n",
      "   â”œâ”€â”€ CombinedCVReport.txt\n",
      "   â”œâ”€â”€ GrowthModelProof.txt\n",
      "   â””â”€â”€ LineagePathDetails/            (Individual plots)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“¤ OUTPUT CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if INPUT_FOLDER:\n",
    "    OUTPUT_FOLDER = INPUT_FOLDER / \"PathResults\"\n",
    "    OUTPUT_FOLDER.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"âœ… Results will be saved to:\")\n",
    "    print(f\"   ğŸ“ {OUTPUT_FOLDER}\\n\")\n",
    "    \n",
    "    # Create subdirectories\n",
    "    (OUTPUT_FOLDER / \"LineagePathDetails\").mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"ğŸ“Š Output structure:\")\n",
    "    print(\"   PathResults/\")\n",
    "    print(\"   â”œâ”€â”€ AllBranchesStats.csv           (â­ Main output)\")\n",
    "    print(\"   â”œâ”€â”€ AllLineagePaths.csv\")\n",
    "    print(\"   â”œâ”€â”€ CombinedCVReport.txt\")\n",
    "    print(\"   â”œâ”€â”€ GrowthModelProof.txt\")\n",
    "    print(\"   â””â”€â”€ LineagePathDetails/            (Individual plots)\")\n",
    "else:\n",
    "    print(\"âš ï¸  Skipped: No input folder selected yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f359a4-1aa0-4b52-ace7-0606da7b83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: load_trackmate_data()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1ï¸âƒ£ TRACKMATE DATA LOADER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def load_trackmate_data(spots_path, edges_path):\n",
    "    \"\"\"\n",
    "    Load and normalize TrackMate spot/edge CSV exports.\n",
    "    \n",
    "    Handles:\n",
    "    - TrackMate's 4-line header format\n",
    "    - Column name variations (POSITION_T vs FRAME)\n",
    "    - Geometric measurements (major/minor axis, circularity, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        spots (DataFrame): Cell measurements per timepoint\n",
    "        edges (DataFrame): Parent-child linkages\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read headers from line 1 (TrackMate format)\n",
    "    with open(spots_path, 'r', encoding='utf-8') as f:\n",
    "        h_spots = f.readline().strip().split(',')\n",
    "    with open(edges_path, 'r', encoding='utf-8') as f:\n",
    "        h_edges = f.readline().strip().split(',')\n",
    "    \n",
    "    # Load data (skip 4-line header)\n",
    "    spots = pd.read_csv(spots_path, names=h_spots, skiprows=4, encoding='utf-8', low_memory=False)\n",
    "    edges = pd.read_csv(edges_path, names=h_edges, skiprows=4, encoding='utf-8', low_memory=False)\n",
    "    \n",
    "    # Clean column names\n",
    "    spots.columns = spots.columns.str.strip()\n",
    "    edges.columns = edges.columns.str.strip()\n",
    "    \n",
    "    # Standardize column names (TrackMate variations)\n",
    "    column_map = {\n",
    "        'ID': 'ID',\n",
    "        'TRACK_ID': 'TRACK_ID',\n",
    "        'AREA': 'AREA',\n",
    "        'POSITION_X': 'POSITION_X',\n",
    "        'POSITION_Y': 'POSITION_Y',\n",
    "        'ELLIPSE_MAJOR': 'MAJOR',\n",
    "        'ELLIPSE_MINOR': 'MINOR',\n",
    "        'POSITION_T': 'FRAME',\n",
    "        'PERIMETER': 'PERIMETER',\n",
    "        'CIRCULARITY': 'CIRCULARITY',\n",
    "        'SOLIDITY': 'SOLIDITY',\n",
    "        'SHAPE_INDEX': 'SHAPE_INDEX',\n",
    "        'ELLIPSE_THETA': 'ANGLE'\n",
    "    }\n",
    "    \n",
    "    # Handle FRAME vs POSITION_T\n",
    "    if 'FRAME' in spots.columns:\n",
    "        column_map.pop('POSITION_T', None)\n",
    "    \n",
    "    edge_map = {\n",
    "        'SPOT_SOURCE_ID': 'SOURCE',\n",
    "        'SPOT_TARGET_ID': 'TARGET'\n",
    "    }\n",
    "    \n",
    "    # Rename\n",
    "    spots = spots.rename(columns={c: column_map[c] for c in spots.columns if c in column_map})\n",
    "    edges = edges.rename(columns={c: edge_map[c] for c in edges.columns if c in edge_map})\n",
    "    \n",
    "    # Convert to numeric\n",
    "    numeric_cols = ['ID', 'TRACK_ID', 'FRAME', 'AREA', 'POSITION_X', 'POSITION_Y', \n",
    "                    'MAJOR', 'MINOR', 'PERIMETER', 'CIRCULARITY', 'SOLIDITY', \n",
    "                    'SHAPE_INDEX', 'ANGLE']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in spots:\n",
    "            spots[col] = pd.to_numeric(spots[col], errors='coerce')\n",
    "    \n",
    "    return spots.dropna(subset=['ID']), edges.dropna(subset=['SOURCE', 'TARGET'])\n",
    "\n",
    "print(\"âœ… Function loaded: load_trackmate_data()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd67f48-e763-46c0-9f0c-2422c5e8add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: parse_filename_metadata()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2ï¸âƒ£ FILENAME METADATA PARSER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def parse_filename_metadata(filename_stem):\n",
    "    \"\"\"\n",
    "    Extract experimental metadata from TrackMate filename.\n",
    "    \n",
    "    Expected format: [Date]_[Type]_[Gene]_[Location]_\n",
    "    Example: 20260116_mutant_mreB__s01_\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'Date': ..., 'Gene': ..., 'Condition': ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    name = filename_stem.replace(\"_spots\", \"\").replace(\"_edges\", \"\")\n",
    "    parts = name.split(\"_\")\n",
    "    \n",
    "    meta = {\n",
    "        'Date': 'Unknown',\n",
    "        'Gene': 'Unknown',\n",
    "        'Condition': 'Unknown'\n",
    "    }\n",
    "    # Safety check: Do we have enough parts?\n",
    "    if len(parts) >= 3:\n",
    "        meta['Date'] = parts[0] # 1st part: Date\n",
    "        strain_type = parts[1] # 2nd part: \"mutant\" or \"wt\"\n",
    "        \n",
    "        if strain_type.lower() in ['wt', 'wildtype', 'control']:\n",
    "            meta['Condition'] = 'Wild Type'\n",
    "            meta['Gene'] = 'WT'\n",
    "        else:\n",
    "            # Assume 3rd part is the gene name (e.g., \"mreB\")\n",
    "            gene_name = parts[2]\n",
    "            meta['Gene'] = gene_name\n",
    "            meta['Condition'] = f\"{gene_name} {strain_type}\"  # e.g., \"mreB mutant\"\n",
    "    \n",
    "    return meta\n",
    "\n",
    "print(\"âœ… Function loaded: parse_filename_metadata()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7d07f8-6e1b-4d25-be73-b7096fac5c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Progress tracker ready!\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¯ PROGRESS TRACKING UTILITIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Simple progress tracker for batch processing\"\"\"\n",
    "    \n",
    "    def __init__(self, total):\n",
    "        self.total = total\n",
    "        self.current = 0\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def update(self, filename):\n",
    "        self.current += 1\n",
    "        elapsed = (datetime.now() - self.start_time).total_seconds()\n",
    "        avg_time = elapsed / self.current if self.current > 0 else 0\n",
    "        remaining = avg_time * (self.total - self.current)\n",
    "        \n",
    "        print(f\"[{self.current}/{self.total}] Processing: {filename}\")\n",
    "        if self.current > 1:\n",
    "            print(f\"   â±ï¸  Avg: {avg_time:.1f}s/file | Est. remaining: {remaining/60:.1f} min\")\n",
    "    \n",
    "    def complete(self):\n",
    "        elapsed = (datetime.now() - self.start_time).total_seconds()\n",
    "        print(f\"\\nâœ… Complete! Total time: {elapsed/60:.1f} minutes\")\n",
    "\n",
    "print(\"âœ… Progress tracker ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a80ac-b418-4151-b7ea-bc6a89fca8e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# â–¶ï¸ Ready to Run!\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "1. **Verify your configuration** (Cell 3) - especially `FRAME_INTERVAL`\n",
    "2. **Check your input folder** (Cell 4) - should show found files\n",
    "3. **Click \"Run All Below\"** or manually execute remaining cells\n",
    "\n",
    "**What happens next:**\n",
    "\n",
    "- Each TrackMate file pair will be processed \n",
    "- Progress updates will show in real-time\n",
    "- Plots will appear inline (if in Jupyter/Colab)\n",
    "- All outputs save to `PathResults/` folder\n",
    "\n",
    "**Troubleshooting:**\n",
    "\n",
    "- **\"No valid pairs found\"** â†’ Check filename format (*_spots.csv / *_edges.csv)\n",
    "- **\"Column not found\"** â†’ Verify TrackMate export settings (need AREA, POSITION_X, etc.)\n",
    "- **\"Low RÂ² warning\"** â†’ Cells aren't growing exponentially (try lowering `THRESH_R2`)\n",
    "\n",
    "---\n",
    "\n",
    "â¬‡ï¸ **Scroll down and run remaining cells** (they contain the analysis functions + main execution loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c29979-d4fd-4d30-89a4-a9477e4a5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: segment_lineages()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3ï¸âƒ£ LINEAGE RECONSTRUCTION (Graph Theory)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def segment_lineages(spots, edges):\n",
    "    \"\"\"\n",
    "    Reconstruct full lineage trees from TrackMate spots + edges.\n",
    "    \n",
    "    Uses NetworkX to:\n",
    "    1. Build a directed graph of all connections\n",
    "    2. Identify connected components (families)\n",
    "    3. Find root nodes (ancestors)\n",
    "    4. Traverse trees to assign Branch IDs and Generations\n",
    "    \n",
    "    Returns:\n",
    "        spots (DataFrame): Enriched with BRANCH_ID, GENERATION, PARENT_BRANCH, ROOT_ID\n",
    "    \"\"\"\n",
    "    \n",
    "    if edges.empty:\n",
    "        return spots\n",
    "        \n",
    "    # Build graph\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in edges.iterrows():\n",
    "        G.add_edge(row['SOURCE'], row['TARGET'])\n",
    "        \n",
    "    branch_map = {}   # spot_id -> branch_id\n",
    "    branch_info = {}  # branch_id -> {gen, parent, root}\n",
    "    \n",
    "    # Process each family tree\n",
    "    for component in nx.weakly_connected_components(G):\n",
    "        subgraph = G.subgraph(component)\n",
    "        \n",
    "        # Find the root (node with no incoming edges)\n",
    "        roots = [n for n in subgraph.nodes if subgraph.in_degree(n) == 0]\n",
    "        if not roots: continue\n",
    "        root = roots[0]\n",
    "        \n",
    "        # Determine root ID (use TRACK_ID if available, else first spot ID)\n",
    "        try: tid = int(spots.loc[spots['ID']==root, 'TRACK_ID'].iloc[0])\n",
    "        except: tid = 999\n",
    "            \n",
    "        root_id = f\"{tid}_0\"  # e.g., \"001\"\n",
    "        \n",
    "        # BFS Traversal to assign branches\n",
    "        # Queue: (current_node, current_branch_id, generation, root_id)\n",
    "        b_id = root_id\n",
    "        queue = [(root, b_id, 1, root_id)]  # Added root_id to queue\n",
    "        branch_map[root] = b_id\n",
    "        branch_info[b_id] = {'gen': 1, 'parent': None, 'root': root_id}\n",
    "        visited = set()\n",
    "        \n",
    "        while queue:\n",
    "            curr, b_id, gen, root_id = queue.pop(0)\n",
    "            if curr in visited: continue\n",
    "            visited.add(curr)\n",
    "            \n",
    "            children = list(subgraph.successors(curr))\n",
    "            \n",
    "            # Case 1: Elongation (1 child) -> Same branch\n",
    "            if len(children) == 1:\n",
    "                branch_map[children[0]] = b_id\n",
    "                queue.append((children[0], b_id, gen, root_id))\n",
    "                \n",
    "            # Case 2: Division (2 children) -> New branches\n",
    "            elif len(children) >= 2:\n",
    "                for i, child in enumerate(children):\n",
    "                    new_b_id = f\"{b_id}_{int(child)}\"\n",
    "                    branch_map[child] = new_b_id\n",
    "                    branch_info[new_b_id] = {'gen': gen + 1, 'parent': b_id, 'root': root_id}\n",
    "                    queue.append((child, new_b_id, gen + 1, root_id))\n",
    "    \n",
    "    # Map back to DataFrame\n",
    "    spots['BRANCH_ID'] = spots['ID'].map(branch_map)\n",
    "    spots['GENERATION'] = spots['BRANCH_ID'].map(lambda x: branch_info.get(x, {}).get('gen', 0))\n",
    "    spots['PARENT_BRANCH'] = spots['BRANCH_ID'].map(lambda x: branch_info.get(x, {}).get('parent', None))\n",
    "    spots['ROOT_ID'] = spots['BRANCH_ID'].map(lambda x: branch_info.get(x, {}).get('root', None))\n",
    "    \n",
    "    return spots.dropna(subset=['BRANCH_ID'])\n",
    "\n",
    "print(\"âœ… Function loaded: segment_lineages()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3494382-1cda-4734-bcd6-49a22a84aece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: calculate_growth_dynamics()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4ï¸âƒ£ GROWTH DYNAMICS (Exponential Fitting)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def calculate_growth_dynamics(times, areas, window_size=3):\n",
    "    \"\"\"\n",
    "    Calculate specific growth rate (k) by fitting exponential curve:\n",
    "    Area(t) = Area_0 * e^(k*t)\n",
    "    \n",
    "    Returns:\n",
    "        specific_rate (float): Overall growth rate k (min^-1)\n",
    "        max_rate (float): Maximum instantaneous rate (sliding window)\n",
    "        r_squared (float): Quality of fit (0.0 - 1.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Deduplicate timepoints (TrackMate artifact handling)\n",
    "    if len(np.unique(times)) < len(times):\n",
    "        _, idx = np.unique(times, return_index=True)\n",
    "        times = times[idx]; areas = areas[idx]\n",
    "        \n",
    "    if len(times) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "        \n",
    "    # Log transform for linear regression: ln(Area) = ln(A0) + k*t\n",
    "    log_areas = np.log(np.maximum(areas, 1e-9))\n",
    "    \n",
    "    # 1. Overall Fit\n",
    "    try:\n",
    "        res_overall = linregress(times, log_areas)\n",
    "        specific_rate = res_overall.slope\n",
    "        r_squared = res_overall.rvalue**2\n",
    "    except:\n",
    "        return np.nan, np.nan, np.nan\n",
    "        \n",
    "    # 2. Max Instantaneous Rate (Sliding Window)\n",
    "    if len(times) < window_size:\n",
    "        max_rate = specific_rate\n",
    "    else:\n",
    "        slopes = []\n",
    "        for i in range(len(times) - window_size + 1):\n",
    "            t_w = times[i : i+window_size]\n",
    "            a_w = log_areas[i : i+window_size]\n",
    "            \n",
    "            if np.std(t_w) < 1e-10: continue\n",
    "            \n",
    "            try:\n",
    "                s = linregress(t_w, a_w).slope\n",
    "                if not np.isnan(s) and not np.isinf(s):\n",
    "                    slopes.append(s)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        max_rate = max(slopes) if slopes else specific_rate\n",
    "        \n",
    "    return specific_rate, max_rate, r_squared\n",
    "\n",
    "print(\"âœ… Function loaded: calculate_growth_dynamics()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d8d15c5-d088-4660-9094-bdc36731fa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: analyze_robust()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5ï¸âƒ£ ROBUST ANALYSIS (Stats & Quality Control)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def analyze_robust(df):\n",
    "    \"\"\"\n",
    "    Aggregates frame-by-frame data into cell-cycle statistics.\n",
    "    Applies quality filters and calculates detailed geometry/growth metrics.\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    # Sort and calculate instantaneous changes\n",
    "    df = df.sort_values(['BRANCH_ID', 'FRAME'])\n",
    "    df['dx'] = df.groupby('BRANCH_ID')['POSITION_X'].diff()\n",
    "    df['dy'] = df.groupby('BRANCH_ID')['POSITION_Y'].diff()\n",
    "    df['step_dist'] = np.sqrt(df['dx']**2 + df['dy']**2)\n",
    "    df['area_change'] = df.groupby('BRANCH_ID')['AREA'].pct_change()\n",
    "    \n",
    "    grouped = df.groupby('BRANCH_ID')\n",
    "    \n",
    "    # [MODIFIED] Aggregation now captures start/end/mean for all geometry cols\n",
    "    agg_dict = {\n",
    "        'GENERATION': 'first', 'FRAME': ['min', 'max'],\n",
    "        'AREA': ['mean', 'first', 'last', 'std'],\n",
    "        'MAJOR': ['mean', 'first', 'last'],\n",
    "        'MINOR': ['mean', 'first', 'last'],\n",
    "        'PERIMETER': ['mean', 'first', 'last'],\n",
    "        'CIRCULARITY': ['mean', 'first', 'last'],\n",
    "        'SOLIDITY': ['mean', 'first', 'last'],\n",
    "        'SHAPE_INDEX': ['mean', 'first', 'last'],\n",
    "        'ANGLE': ['mean', 'first', 'last'],\n",
    "        'step_dist': 'max', 'area_change': 'max',\n",
    "        'PARENT_BRANCH': 'first', 'ROOT_ID': 'first'\n",
    "    }\n",
    "\n",
    "    # Only aggregate columns that actually exist in input\n",
    "    agg_dict = {k: v for k, v in agg_dict.items() if k in df.columns or k in ['step_dist', 'area_change']}\n",
    "\n",
    "    cell_meta = grouped.agg(agg_dict)\n",
    "    \n",
    "    # [MODIFIED] Dynamic flattening and renaming of columns\n",
    "    # Flatten MultiIndex columns: ('AREA', 'mean') -> 'AREA_mean'\n",
    "    cell_meta.columns = [f\"{c[0]}_{c[1]}\" if c[1] else c[0] for c in cell_meta.columns]\n",
    "    \n",
    "    rename_map = {}\n",
    "    for col in cell_meta.columns:\n",
    "        if '_first' in col: \n",
    "            rename_map[col] = col.replace('_first', '_Start')\n",
    "            if col == 'AREA_first': rename_map[col] = 'Start_Area'\n",
    "            elif col == 'MAJOR_first': rename_map[col] = 'Start_Major'\n",
    "            elif col == 'MINOR_first': rename_map[col] = 'Start_Minor'\n",
    "            \n",
    "        elif '_last' in col: \n",
    "            rename_map[col] = col.replace('_last', '_End')\n",
    "            if col == 'AREA_last': rename_map[col] = 'End_Area'\n",
    "            elif col == 'MAJOR_last': rename_map[col] = 'End_Major'\n",
    "            elif col == 'MINOR_last': rename_map[col] = 'End_Minor'\n",
    "\n",
    "        elif '_mean' in col: \n",
    "            rename_map[col] = col.replace('_mean', '_Mean')\n",
    "            if col == 'AREA_mean': rename_map[col] = 'Mean_Area'\n",
    "            elif col == 'MAJOR_mean': rename_map[col] = 'Mean_Major'\n",
    "            elif col == 'MINOR_mean': rename_map[col] = 'Mean_Minor'\n",
    "    \n",
    "    # Explicit mapping for other required columns\n",
    "    rename_map.update({\n",
    "        'GENERATION_first': 'Gen', \n",
    "        'FRAME_min': 'Start_Frame', \n",
    "        'FRAME_max': 'End_Frame',\n",
    "        'AREA_std': 'Area_StD', \n",
    "        'step_dist_max': 'Max_Step', \n",
    "        'area_change_max': 'Max_Growth_Pct',\n",
    "        'PARENT_BRANCH_first': 'Parent_ID', \n",
    "        'ROOT_ID_first': 'Root_ID'\n",
    "    })\n",
    "    \n",
    "    cell_meta = cell_meta.rename(columns=rename_map)\n",
    "    cell_meta['Duration'] = cell_meta['End_Frame'] - cell_meta['Start_Frame']\n",
    "    cell_meta['Duration_Minutes'] = cell_meta['Duration'] * FRAME_INTERVAL\n",
    "    \n",
    "    if 'Mean_Major' in cell_meta.columns and 'Mean_Minor' in cell_meta.columns:\n",
    "        cell_meta['Mean_AR'] = cell_meta['Mean_Major'] / np.maximum(cell_meta['Mean_Minor'], 1e-9)\n",
    "    \n",
    "    children_map = df.dropna(subset=['PARENT_BRANCH']).groupby('PARENT_BRANCH')['BRANCH_ID'].unique().to_dict()\n",
    "    \n",
    "    # [NEW] Pre-calculate linked daughter sizes for efficiency\n",
    "    start_major_map = cell_meta['Start_Major'].to_dict() if 'Start_Major' in cell_meta.columns else {}\n",
    "    start_area_map = cell_meta['Start_Area'].to_dict() if 'Start_Area' in cell_meta.columns else {}\n",
    "\n",
    "    for pid, row in cell_meta.iterrows():\n",
    "        b_data = df[df['BRANCH_ID'] == pid]\n",
    "        \n",
    "        # [FIX] Capture k_max_frame (sliding window max)\n",
    "        k_frame, k_max_frame, r2 = calculate_growth_dynamics(b_data['FRAME'].values, b_data['AREA'].values)\n",
    "        \n",
    "        k_min = k_frame / FRAME_INTERVAL if pd.notna(k_frame) else np.nan\n",
    "        k_max_min = k_max_frame / FRAME_INTERVAL if pd.notna(k_max_frame) else np.nan  # <--- NEW\n",
    "        \n",
    "        doubling_time = (np.log(2)/k_min) if (pd.notna(k_min) and k_min > 1e-6) else np.nan\n",
    "\n",
    "        jump_limit = max(row.get('End_Major', 5.0) * THRESH_JUMP_MULT, 5.0)\n",
    "        min_dur = MIN_DURATION_MOTHER if row['Gen'] == 1 else MIN_DURATION_DAUGHTER\n",
    "        is_exponential = (pd.notna(r2) and r2 >= THRESH_R2)\n",
    "        \n",
    "        is_valid = (\n",
    "            (row['Duration'] >= min_dur) and \n",
    "            (row['Max_Step'] < jump_limit) and \n",
    "            (row['Max_Growth_Pct'] < THRESH_GROWTH) and \n",
    "            is_exponential\n",
    "        )\n",
    "        \n",
    "        children = children_map.get(pid, [])\n",
    "        did_divide = len(children) >= 2\n",
    "        is_clean, balance, cons = False, np.nan, np.nan\n",
    "        \n",
    "        # [NEW] Calculate Just After Division (Linked Daughter Start)\n",
    "        linked_start_major = np.nan\n",
    "        linked_start_area = np.nan\n",
    "        \n",
    "        if did_divide:\n",
    "            d_metrics = []\n",
    "            d_majors = []\n",
    "            d_areas = []\n",
    "            \n",
    "            for cid in children:\n",
    "                if cid not in cell_meta.index: continue\n",
    "                c_row = cell_meta.loc[cid]\n",
    "                # Fix 3: Safety check for empty frames\n",
    "                m_end = df[(df['BRANCH_ID']==pid) & (df['FRAME']==row['End_Frame'])]\n",
    "                d_start = df[(df['BRANCH_ID']==cid) & (df['FRAME']==c_row['Start_Frame'])]\n",
    "                \n",
    "                if m_end.empty or d_start.empty: continue\n",
    "                \n",
    "                dist = np.sqrt((d_start.iloc[0]['POSITION_X'] - m_end.iloc[0]['POSITION_X'])**2 +\n",
    "                               (d_start.iloc[0]['POSITION_Y'] - m_end.iloc[0]['POSITION_Y'])**2)\n",
    "                \n",
    "                d_metrics.append({'area': d_start.iloc[0]['AREA'], 'dist': dist, 'dur': c_row['Duration']})\n",
    "                \n",
    "                # Collect daughter starts\n",
    "                if cid in start_major_map: d_majors.append(start_major_map[cid])\n",
    "                if cid in start_area_map: d_areas.append(start_area_map[cid])\n",
    "            \n",
    "            # Calculate Linked Means\n",
    "            if d_majors: linked_start_major = np.mean(d_majors)\n",
    "            if d_areas: linked_start_area = np.mean(d_areas)\n",
    "            \n",
    "            if len(d_metrics) >= 2:\n",
    "                d_metrics.sort(key=lambda x: x['area'], reverse=True)\n",
    "                d1, d2 = d_metrics[0], d_metrics[1]\n",
    "                \n",
    "                balance = d2['area'] / d1['area']\n",
    "                cons = (d1['area'] + d2['area']) / row['End_Area']\n",
    "                \n",
    "                is_clean = (\n",
    "                    (balance >= THRESH_BALANCE) and \n",
    "                    (THRESH_CONS_MIN <= cons <= THRESH_CONS_MAX) and \n",
    "                    (d1['dist'] < jump_limit) and (d2['dist'] < jump_limit)\n",
    "                )\n",
    "\n",
    "        out_row = {\n",
    "            'Cell_ID': pid, \n",
    "            'Generation': row['Gen'],\n",
    "            'Duration': row['Duration'], \n",
    "            'Duration_Minutes': round(row['Duration_Minutes'], 2),\n",
    "            'Start_Frame': row['Start_Frame'], \n",
    "            'End_Frame': row['End_Frame'],\n",
    "            'Did_Divide': did_divide, \n",
    "            'Is_Valid_Track': is_valid, \n",
    "            'Is_Clean_Division': is_clean,\n",
    "            'Balance': round(balance, 3), \n",
    "            'Conservation': round(cons, 3),\n",
    "            'Mean_Area': round(row.get('Mean_Area', np.nan), 3), \n",
    "            'Area_StD': round(row.get('Area_StD', np.nan), 3),\n",
    "            'Mean_Major': round(row.get('Mean_Major', np.nan), 3), \n",
    "            'Mean_Minor': round(row.get('Mean_Minor', np.nan), 3), \n",
    "            'Mean_AR': round(row.get('Mean_AR', np.nan), 3),\n",
    "            'Growth_Rate_k_min': round(k_min, 5), \n",
    "            'Max_Instant_k_min': round(k_max_min, 5), # <--- NEW\n",
    "            'Doubling_Time_min': round(doubling_time, 2),\n",
    "            'Growth_R2': round(r2, 4) if pd.notna(r2) else np.nan,\n",
    "            'Parent_ID': row['Parent_ID'], \n",
    "            'Root_ID': row['Root_ID'],\n",
    "            'Linked_Daughter_Start_Major': round(linked_start_major, 3), # <--- NEW\n",
    "            'Linked_Daughter_Start_Area': round(linked_start_area, 3)    # <--- NEW\n",
    "        }\n",
    "        \n",
    "        # [MODIFIED] Dynamic add of Start/Mean/End geometry to output\n",
    "        geo_cols = ['Area', 'Major', 'Minor', 'Perimeter', 'Circularity', 'Solidity', 'Shape_Index', 'Angle']\n",
    "        prefixes = ['Start', 'Mean', 'End']\n",
    "        \n",
    "        for g in geo_cols:\n",
    "            for p in prefixes:\n",
    "                # Map logical name to actual column name in cell_meta\n",
    "                # Special cases handled in rename_map, others follow Pattern_Metric\n",
    "                \n",
    "                col_name = f\"{p}_{g}\"\n",
    "                if col_name == 'Start_Area': col_name = 'Start_Area' # already correct\n",
    "                # ... (add other explicit mapping if rename_map logic above missed any) ...\n",
    "                \n",
    "                # Try to find the value in row\n",
    "                val = np.nan\n",
    "                if col_name in row:\n",
    "                    val = row[col_name]\n",
    "                # Fallback check for original names if not renamed\n",
    "                elif f\"{g.upper()}_{p.lower()}\" in row: \n",
    "                    val = row[f\"{g.upper()}_{p.lower()}\"]\n",
    "\n",
    "                out_row[f\"{p}_{g}\"] = round(val, 3) if pd.notna(val) else np.nan\n",
    "\n",
    "        stats.append(out_row)\n",
    "        \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "print(\"âœ… Function loaded: analyze_robust()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa12707-7cb1-4558-a532-c7eaf6785b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: prove_growth_model()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6ï¸âƒ£ GROWTH MODEL CLASSIFICATION (Adder/Sizer/Timer)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def prove_growth_model(df, filename_stem, output_path):\n",
    "    \"\"\"\n",
    "    Calculates slope of Added Size vs Birth Size.\n",
    "    \n",
    "    Classifies growth behavior:\n",
    "    - Adder: Slope â‰ˆ 0 (Add constant amount)\n",
    "    - Timer: Slope â‰ˆ 1 (Grow for constant time)\n",
    "    - Sizer: Slope â‰ˆ -1 (Grow to target size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for valid tracks that divided\n",
    "    valid = df[(df['Is_Valid_Track']) & (df['Did_Divide'])].copy()\n",
    "    if len(valid) < 10: return\n",
    "    \n",
    "    # Ensure columns exist\n",
    "    if 'End_Area' not in valid or 'Start_Area' not in valid: return\n",
    "    \n",
    "    valid['Added_Area'] = valid['End_Area'] - valid['Start_Area']\n",
    "    \n",
    "    # If Major axis available, check length model too\n",
    "    has_length = 'End_Major' in valid and 'Start_Major' in valid\n",
    "    if has_length:\n",
    "        valid['Added_Length'] = valid['End_Major'] - valid['Start_Major']\n",
    "    \n",
    "    try:\n",
    "        slope_area, _, r_area, _, _ = linregress(valid['Start_Area'], valid['Added_Area'])\n",
    "        \n",
    "        slope_len = np.nan\n",
    "        r_len = np.nan\n",
    "        if has_length:\n",
    "            slope_len, _, r_len, _, _ = linregress(valid['Start_Major'], valid['Added_Length'])\n",
    "        \n",
    "        def get_model(s):\n",
    "            if np.isnan(s): return \"N/A\"\n",
    "            if -0.3 <= s <= 0.3: return \"ADDER\"\n",
    "            elif s > 0.7: return \"TIMER\"\n",
    "            elif s < -0.7: return \"SIZER\"\n",
    "            elif 0.3 < s <= 0.7: return \"Timer-like Adder\"\n",
    "            else: return \"Sizer-like Adder\"\n",
    "\n",
    "        report_line = (\n",
    "            f\"FILE: {filename_stem}\\n\"\n",
    "            f\"  N_Cells: {len(valid)}\\n\"\n",
    "            f\"  AREA Slope: {slope_area:.3f} (R2={r_area**2:.2f}) -> {get_model(slope_area)}\\n\"\n",
    "        )\n",
    "        \n",
    "        if has_length:\n",
    "            report_line += f\"  LENGTH Slope: {slope_len:.3f} (R2={r_len**2:.2f}) -> {get_model(slope_len)}\\n\"\n",
    "            \n",
    "        report_line += f\"{'-'*40}\\n\"\n",
    "        \n",
    "        with open(output_path, \"a\") as f: f.write(report_line)\n",
    "        print(f\"   Growth model proof added for {filename_stem}\")\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(f\"   Growth model calc failed: {e}\")\n",
    "\n",
    "print(\"âœ… Function loaded: prove_growth_model()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f657ba-884b-4439-941a-31429d922d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: extract_lineage_paths()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 7ï¸âƒ£ LINEAGE PATH EXTRACTOR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def extract_lineage_paths(stats_df, max_depth=5):\n",
    "    \"\"\"\n",
    "    Build ALL rootâ†’leaf paths, including partial and non-dividing lineages.\n",
    "    Returns DataFrame with matched (t1, t2, t3, ...) tuples per lineage.\n",
    "    \"\"\"\n",
    "\n",
    "    children_map = stats_df[stats_df['Parent_ID'].notna()].groupby('Parent_ID')['Cell_ID'].apply(list).to_dict()\n",
    "    roots = stats_df[stats_df['Generation'] == 1]['Cell_ID'].tolist()\n",
    "    \n",
    "    all_paths = []\n",
    "    \n",
    "    def dfs_paths(current_id, path_so_far, depth):\n",
    "        \"\"\"Recursive DFS that stops at leaves OR max_depth\"\"\"\n",
    "        \n",
    "        current_path = path_so_far + [current_id]\n",
    "        children = children_map.get(current_id, [])\n",
    "        \n",
    "        # TERMINAL: Leaf node or max depth\n",
    "        if not children or depth >= max_depth:\n",
    "            all_paths.append(current_path)\n",
    "            return\n",
    "        \n",
    "        # RECURSIVE: Visit all children (branching)\n",
    "        for child_id in children:\n",
    "            dfs_paths(child_id, current_path, depth + 1)\n",
    "    \n",
    "    for root_id in roots:\n",
    "        dfs_paths(root_id, [], 1)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    path_records = []\n",
    "    for path in all_paths:\n",
    "        record = {\n",
    "            'Lineage_Path_ID': ' â†’ '.join(path),\n",
    "            'Path_Length': len(path),\n",
    "            'Root_ID': path[0]\n",
    "        }\n",
    "        \n",
    "        # Extract metrics for each node\n",
    "        for i, branch_id in enumerate(path, start=1):\n",
    "            row = stats_df[stats_df['Cell_ID'] == branch_id].iloc[0]\n",
    "            record[f't{i}'] = row['Duration']\n",
    "            record[f't{i}_min'] = row['Duration_Minutes']\n",
    "            record[f'Start_Frame_{i}'] = row['Start_Frame']\n",
    "            record[f'End_Frame_{i}'] = row['End_Frame']\n",
    "            record[f'Did_Divide_{i}'] = row['Did_Divide']\n",
    "            record[f'Is_Clean_Division_{i}'] = row['Is_Clean_Division']\n",
    "            record[f'Is_Valid_Track_{i}'] = row['Is_Valid_Track']\n",
    "        \n",
    "        # Quality tier\n",
    "        num_divisions = sum(record.get(f'Did_Divide_{i}', False) for i in range(1, len(path)))\n",
    "        num_clean = sum(record.get(f'Is_Clean_Division_{i}', False) for i in range(1, len(path)))\n",
    "        \n",
    "        if num_divisions == 0:\n",
    "            tier = 'No_Division'\n",
    "        elif num_clean == num_divisions and len(path) >= 3:\n",
    "            tier = 'Gold'\n",
    "        elif num_clean >= num_divisions * 0.5:\n",
    "            tier = 'Silver'\n",
    "        else:\n",
    "            tier = 'Bronze'\n",
    "        \n",
    "        record['Quality_Tier'] = tier\n",
    "        record['Num_Divisions'] = num_divisions\n",
    "        record['Num_Clean_Divisions'] = num_clean\n",
    "        \n",
    "        path_records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(path_records)\n",
    "\n",
    "print(\"âœ… Function loaded: extract_lineage_paths()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3504210-b2c0-4ee1-8b89-23d6d8dcf1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: calculate_cv_statistics()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 8ï¸âƒ£ CV DIAGNOSTIC REPORTER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def calculate_cv_statistics(stats_df, output_path):\n",
    "    \"\"\"\n",
    "    Generates a Coefficient of Variation (CV) report to assess\n",
    "    population heterogeneity and data quality.\n",
    "    \"\"\"\n",
    "    report = [\"COEFFICIENT OF VARIATION (CV) DIAGNOSTIC REPORT\", \"=\"*50, \"\"]\n",
    "    clean = stats_df[(stats_df['Is_Valid_Track']) & (stats_df['Is_Clean_Division'])]\n",
    "    \n",
    "    if clean.empty:\n",
    "        with open(output_path, 'w') as f: f.write(\"No clean events found.\")\n",
    "        return\n",
    "\n",
    "    # 1. CV by Generation\n",
    "    report.append(f\"{'Gen':<6} {'N':<6} {'Mean(min)':<12} {'CV(%)':<8} {'Status'}\")\n",
    "    for gen in sorted(clean['Generation'].unique()):\n",
    "        data = clean[clean['Generation'] == gen]['Duration_Minutes']\n",
    "        if len(data) < 3: continue\n",
    "        cv = (data.std() / data.mean() * 100)\n",
    "        status = \"âœ“\" if cv < 30 else (\"âš \" if cv < 40 else \"âŒ\")\n",
    "        report.append(f\"{gen:<6} {len(data):<6} {data.mean():<12.2f} {cv:<8.2f} {status}\")\n",
    "\n",
    "    # 2. Overall Stats\n",
    "    all_d = clean['Duration_Minutes']\n",
    "    report.append(f\"\\nOverall CV: {(all_d.std()/all_d.mean()*100):.2f}% (N={len(all_d)})\")\n",
    "    \n",
    "    # 3. Normality Check\n",
    "    try:\n",
    "        skewness = skew(all_d); kurt = kurtosis(all_d)\n",
    "        report.append(f\"Skewness: {skewness:.3f} (Ideal: 0)\")\n",
    "        if abs(skewness) > 1: report.append(\"âš  WARNING: Distribution is skewed (check for mixed populations).\")\n",
    "    except: pass\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f: f.write('\\n'.join(report))\n",
    "    print(f\"   Diagnostic report saved to {output_path}\")\n",
    "\n",
    "print(\"âœ… Function loaded: calculate_cv_statistics()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6bad379-25ff-4f79-81df-ab9c088df1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Function loaded: plot_lineage_path_details(), plot_lineage_tree_network()\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 9ï¸âƒ£ VISUALIZATION DASHBOARDS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def plot_robust_dashboard(stats, lineages, out_file):\n",
    "    \"\"\"\n",
    "    Summary dashboard:\n",
    "    1. Time to Division (Boxplot)\n",
    "    2. Physics Check (Balance vs Conservation)\n",
    "    3. Data Yield (Clean vs Rejected)\n",
    "    4. Representative Trace Examples\n",
    "    \"\"\"\n",
    "    valid_stats = stats[stats['Is_Valid_Track'] == True]\n",
    "    clean_stats = stats[stats['Is_Clean_Division'] == True]\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = gridspec.GridSpec(3, 3, height_ratios=[1, 1, 1.5])\n",
    "    \n",
    "    # Plot 1: Time to Division\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    if not clean_stats.empty:\n",
    "        sns.boxplot(data=clean_stats[clean_stats['Generation'] <= 4], \n",
    "                   x='Generation', y='Duration_Minutes', ax=ax1, palette=\"Set2\")\n",
    "    ax1.set_title(\"Time to Division (Clean Events Only)\")\n",
    "    ax1.set_ylabel(\"Minutes\")\n",
    "    \n",
    "    # Plot 2: Physics Check\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    if not valid_stats.empty:\n",
    "        sns.scatterplot(data=valid_stats, x='Conservation', y='Balance', \n",
    "                       hue='Generation', alpha=0.6, ax=ax2)\n",
    "        rect = plt.Rectangle((THRESH_CONS_MIN, THRESH_BALANCE), \n",
    "                             THRESH_CONS_MAX - THRESH_CONS_MIN, \n",
    "                             1.0 - THRESH_BALANCE, \n",
    "                             linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "    ax2.set_title(\"Physics Check (Green Box = Clean)\")\n",
    "    \n",
    "    # Plot 3: Yield\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    counts = stats[stats['Generation'] <= 4].groupby(['Generation', 'Is_Clean_Division']).size().unstack(fill_value=0)\n",
    "    if not counts.empty:\n",
    "        counts.plot(kind='bar', stacked=True, ax=ax3, color=['red', 'green'])\n",
    "    ax3.set_title(\"Data Yield: Clean vs Rejected\")\n",
    "    \n",
    "    # Plot 4: Representative Traces\n",
    "    def plot_trace(ax, pid, gen):\n",
    "        m = lineages[lineages['BRANCH_ID'] == pid].sort_values('FRAME')\n",
    "        if m.empty: return\n",
    "        \n",
    "        try:\n",
    "            gid = m.iloc[0]['PARENT_BRANCH']\n",
    "            if pd.notna(gid):\n",
    "                g = lineages[lineages['BRANCH_ID'] == gid].sort_values('FRAME')\n",
    "                if not g.empty: ax.plot(g['FRAME'], g['AREA'], c='lightgray', ls='--', lw=2)\n",
    "        except: pass\n",
    "        \n",
    "        ax.plot(m['FRAME'], m['AREA'], c='blue', lw=2, label=f'Gen {gen}')\n",
    "        \n",
    "        children = lineages[lineages['PARENT_BRANCH'] == pid]['BRANCH_ID'].unique()\n",
    "        colors = ['red', 'orange']\n",
    "        for i, c_id in enumerate(children[:2]):\n",
    "            c = lineages[lineages['BRANCH_ID'] == c_id].sort_values('FRAME')\n",
    "            if not c.empty:\n",
    "                ax.plot(c['FRAME'], c['AREA'], c=colors[i], label=f'Gen {gen+1}')\n",
    "            \n",
    "        ax.set_title(f\"Gen {gen} Clean Event\\nID: {pid}\", fontsize=9)\n",
    "\n",
    "    for i, gen in enumerate([1, 2, 3]):\n",
    "        ax = fig.add_subplot(gs[2, i])\n",
    "        candidates = clean_stats[clean_stats['Generation'] == gen]\n",
    "        if not candidates.empty:\n",
    "            median_dur = candidates['Duration'].median()\n",
    "            best_row = candidates.iloc[(candidates['Duration'] - median_dur).abs().argsort()[:1]]\n",
    "            plot_trace(ax, best_row.iloc[0]['Cell_ID'], gen)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"No Clean Events\", ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_lineage_tree_gallery(lineage_paths_df, stats_df, lineages_df, out_file, n_examples=6):\n",
    "    \"\"\"\n",
    "    Visualize complete lineage trees from root to leaves.\n",
    "    Shows area traces over time with color-coded branches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select diverse examples (mix of Gold, Silver, Bronze, No_Division)\n",
    "    examples = []\n",
    "    \n",
    "    for tier in ['Gold', 'Silver', 'Bronze', 'No_Division']:\n",
    "        subset = lineage_paths_df[lineage_paths_df['Quality_Tier'] == tier]\n",
    "        if not subset.empty:\n",
    "            # Group by Root_ID to get complete trees\n",
    "            roots = subset['Root_ID'].unique()\n",
    "            if len(roots) > 0:\n",
    "                examples.append((tier, roots[0]))\n",
    "    \n",
    "    # Take up to n_examples\n",
    "    examples = examples[:n_examples]\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"   No lineages to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Create figure\n",
    "    n_cols = 3\n",
    "    n_rows = (len(examples) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 6*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (tier, root_id) in enumerate(examples):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get all paths from this root\n",
    "        root_paths = lineage_paths_df[lineage_paths_df['Root_ID'] == root_id]\n",
    "        \n",
    "        # Get all branches in this tree\n",
    "        all_branches = set()\n",
    "        for _, path_row in root_paths.iterrows():\n",
    "            path_id = path_row['Lineage_Path_ID']\n",
    "            branches = path_id.split(' â†’ ')\n",
    "            all_branches.update(branches)\n",
    "        \n",
    "        # Color map for generations\n",
    "        gen_colors = {1: 'blue', 2: 'red', 3: 'orange', 4: 'purple', 5: 'green'}\n",
    "        \n",
    "        # Plot each branch\n",
    "        plotted_gens = set()\n",
    "        for branch_id in all_branches:\n",
    "            branch_data = lineages_df[lineages_df['BRANCH_ID'] == branch_id].sort_values('FRAME')\n",
    "            if branch_data.empty:\n",
    "                continue\n",
    "            \n",
    "            gen = branch_data.iloc[0]['GENERATION']\n",
    "            color = gen_colors.get(gen, 'gray')\n",
    "            \n",
    "            # Check if this branch divided\n",
    "            branch_stats = stats_df[stats_df['Cell_ID'] == branch_id]\n",
    "            if not branch_stats.empty:\n",
    "                did_divide = branch_stats.iloc[0]['Did_Divide']\n",
    "                is_clean = branch_stats.iloc[0]['Is_Clean_Division']\n",
    "                linestyle = '-' if is_clean else '--'\n",
    "                linewidth = 2.5 if did_divide else 1.5\n",
    "                alpha = 1.0 if is_clean else 0.5\n",
    "            else:\n",
    "                linestyle = '-'\n",
    "                linewidth = 1.5\n",
    "                alpha = 0.7\n",
    "            \n",
    "            label = f'Gen {gen}' if gen not in plotted_gens else ''\n",
    "            ax.plot(branch_data['FRAME'], branch_data['AREA'], \n",
    "                   color=color, linestyle=linestyle, linewidth=linewidth, \n",
    "                   alpha=alpha, label=label)\n",
    "            \n",
    "            plotted_gens.add(gen)\n",
    "        \n",
    "        # Add title with statistics\n",
    "        n_paths = len(root_paths)\n",
    "        max_gen = root_paths['Path_Length'].max()\n",
    "        n_divisions = root_paths['Num_Divisions'].max()\n",
    "        \n",
    "        ax.set_title(f'{tier} Tree: {root_id}\\n{n_paths} paths, {n_divisions} divs, {max_gen} gens', \n",
    "                    fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Frame')\n",
    "        ax.set_ylabel('Area (pixelsÂ²)')\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Legend\n",
    "        if len(plotted_gens) > 0:\n",
    "            ax.legend(fontsize=8, loc='best')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(examples), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   Saved lineage tree gallery to {out_file.name}\")\n",
    "\n",
    "\n",
    "def plot_lineage_path_details(lineage_paths_df, stats_df, lineages_df, out_dir, max_paths=20):\n",
    "    \"\"\"\n",
    "    Create individual detailed plots for specific lineage paths.\n",
    "    One plot per path showing the complete motherâ†’daughterâ†’granddaughter trace.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select paths to visualize (prioritize Gold, then Silver)\n",
    "    gold = lineage_paths_df[lineage_paths_df['Quality_Tier'] == 'Gold'].head(max_paths // 2)\n",
    "    silver = lineage_paths_df[lineage_paths_df['Quality_Tier'] == 'Silver'].head(max_paths // 2)\n",
    "    selected = pd.concat([gold, silver])\n",
    "    \n",
    "    if selected.empty:\n",
    "        print(\"   No high-quality paths to visualize.\")\n",
    "        return\n",
    "    \n",
    "    detail_dir = out_dir / \"Lineage_Path_Details\"\n",
    "    detail_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for idx, row in selected.iterrows():\n",
    "        path_id = row['Lineage_Path_ID']\n",
    "        branches = path_id.split(' â†’ ')\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # LEFT PANEL: Area over time\n",
    "        colors = ['blue', 'red', 'orange', 'purple', 'green']\n",
    "        for i, branch_id in enumerate(branches):\n",
    "            branch_data = lineages_df[lineages_df['BRANCH_ID'] == branch_id].sort_values('FRAME')\n",
    "            if branch_data.empty:\n",
    "                continue\n",
    "            \n",
    "            branch_stats = stats_df[stats_df['Cell_ID'] == branch_id]\n",
    "            if not branch_stats.empty:\n",
    "                is_clean = branch_stats.iloc[0]['Is_Clean_Division']\n",
    "                marker = 'o' if is_clean else 'x'\n",
    "                linewidth = 2.5 if is_clean else 1.5\n",
    "            else:\n",
    "                marker = 'o'\n",
    "                linewidth = 1.5\n",
    "            \n",
    "            ax1.plot(branch_data['FRAME'], branch_data['AREA'], \n",
    "                    color=colors[i % len(colors)], linewidth=linewidth,\n",
    "                    marker=marker, markersize=3, markevery=5,\n",
    "                    label=f'Node {i+1} (Gen {i+1})')\n",
    "            \n",
    "            # EXPONENTIAL Fit Visualization\n",
    "            if len(branch_data) >= 2:\n",
    "                try:\n",
    "                    # Log transform for regression\n",
    "                    log_areas = np.log(branch_data['AREA'] + 1e-9)\n",
    "                    slope, intercept, _, _, _ = linregress(branch_data['FRAME'], log_areas)\n",
    "                    \n",
    "                    # Back-transform for plotting: Area = exp(mt + c)\n",
    "                    fit_vals = np.exp(slope * branch_data['FRAME'] + intercept)\n",
    "                    \n",
    "                    ax1.plot(branch_data['FRAME'], fit_vals, 'k--', linewidth=1.5, alpha=0.5, \n",
    "                             label=f'k={slope:.3f}')\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Mark division events with gold stars\n",
    "            if not branch_stats.empty and branch_stats.iloc[0]['Did_Divide']:\n",
    "                end_frame = branch_stats.iloc[0]['End_Frame']\n",
    "                end_area = branch_data[branch_data['FRAME'] == end_frame]['AREA'].values\n",
    "                if len(end_area) > 0:\n",
    "                    ax1.scatter([end_frame], [end_area[0]], s=200, \n",
    "                              marker='*', color='gold', edgecolor='black', \n",
    "                              linewidth=2, zorder=10)\n",
    "        \n",
    "        ax1.set_xlabel('Frame', fontsize=12)\n",
    "        ax1.set_ylabel('Area (pixelsÂ²)', fontsize=12)\n",
    "        ax1.set_title(f'Lineage Path: {path_id[:50]}...', fontsize=10, fontweight='bold')\n",
    "        ax1.legend(fontsize=10)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # RIGHT PANEL: Statistics table\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Build table data\n",
    "        table_data = []\n",
    "        table_data.append(['Node', 'GrowthRate', 'Did Div?', 'Clean?', 'Balance', 'Conservation'])\n",
    "        \n",
    "        for i, branch_id in enumerate(branches):\n",
    "            branch_stats = stats_df[stats_df['Cell_ID'] == branch_id]\n",
    "            if branch_stats.empty:\n",
    "                continue\n",
    "            \n",
    "            s = branch_stats.iloc[0]\n",
    "            # Use new Growth_Rate if available, else standard placeholder\n",
    "            gr = s['Growth_Rate_k_min'] if 'Growth_Rate_k_min' in s else 0\n",
    "            \n",
    "            table_data.append([\n",
    "                f'Gen {i+1}',\n",
    "                f'{gr:.4f}',\n",
    "                'âœ“' if s['Did_Divide'] else 'âœ—',\n",
    "                'âœ“' if s['Is_Clean_Division'] else 'âœ—',\n",
    "                f'{s[\"Balance\"]:.2f}' if pd.notna(s['Balance']) else 'â€”',\n",
    "                f'{s[\"Conservation\"]:.2f}' if pd.notna(s['Conservation']) else 'â€”'\n",
    "            ])\n",
    "        \n",
    "        table = ax2.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                         colWidths=[0.15, 0.20, 0.15, 0.15, 0.15, 0.2])\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        # Color header row\n",
    "        for i in range(6):\n",
    "            table[(0, i)].set_facecolor('#4CAF50')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        # Color quality indicators\n",
    "        for i in range(1, len(table_data)):\n",
    "            if table_data[i][3] == 'âœ“':\n",
    "                table[(i, 3)].set_facecolor('#C8E6C9')\n",
    "            else:\n",
    "                table[(i, 3)].set_facecolor('#FFCDD2')\n",
    "        \n",
    "        ax2.set_title(f'Path Quality: {row[\"Quality_Tier\"]} | '\n",
    "                     f'{row[\"Num_Clean_Divisions\"]}/{row[\"Num_Divisions\"]} clean divisions',\n",
    "                     fontsize=11, fontweight='bold', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Safe filename\n",
    "        safe_name = path_id.replace(' â†’ ', '_').replace('/', '_')[:100]\n",
    "        plt.savefig(detail_dir / f'Path_{idx}_{safe_name}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"   Saved {len(selected)} detailed path plots to {detail_dir.name}/\")\n",
    "\n",
    "\n",
    "def plot_lineage_tree_network(lineage_paths_df, stats_df, out_file, root_id=None):\n",
    "    \"\"\"\n",
    "    Create a network/tree diagram showing the branching structure.\n",
    "    Uses NetworkX to layout the tree properly.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no root specified, pick the most complete one\n",
    "    if root_id is None:\n",
    "        complete = lineage_paths_df.sort_values('Path_Length', ascending=False)\n",
    "        if complete.empty:\n",
    "            return\n",
    "        root_id = complete.iloc[0]['Root_ID']\n",
    "    \n",
    "    # Get all paths from this root\n",
    "    root_paths = lineage_paths_df[lineage_paths_df['Root_ID'] == root_id]\n",
    "    if root_paths.empty:\n",
    "        return\n",
    "    \n",
    "    # Build directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add all edges from paths\n",
    "    for _, path_row in root_paths.iterrows():\n",
    "        path_id = path_row['Lineage_Path_ID']\n",
    "        branches = path_id.split(' â†’ ')\n",
    "        \n",
    "        for i in range(len(branches) - 1):\n",
    "            parent = branches[i]\n",
    "            child = branches[i + 1]\n",
    "            G.add_edge(parent, child)\n",
    "    \n",
    "    # Node attributes from stats\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        node_stats = stats_df[stats_df['Cell_ID'] == node]\n",
    "        if not node_stats.empty:\n",
    "            s = node_stats.iloc[0]\n",
    "            gen = s['Generation']\n",
    "            \n",
    "            # Color by generation\n",
    "            color_map = {1: '#2196F3', 2: '#F44336', 3: '#FF9800', 4: '#9C27B0', 5: '#4CAF50'}\n",
    "            color = color_map.get(gen, 'gray')\n",
    "            \n",
    "            # Size by duration\n",
    "            size = 300 + s['Duration'] * 20\n",
    "            \n",
    "            node_colors.append(color)\n",
    "            node_sizes.append(size)\n",
    "        else:\n",
    "            node_colors.append('gray')\n",
    "            node_sizes.append(300)\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # Try hierarchical layout if available\n",
    "    try:\n",
    "        pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, arrows=True, arrowsize=20, \n",
    "                           arrowstyle='->', width=2, alpha=0.6, \n",
    "                           edge_color='gray')\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_color=node_colors, \n",
    "                           node_size=node_sizes, alpha=0.9, \n",
    "                           edgecolors='black', linewidths=2)\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {node: node.split('_')[-1][:10] for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels, ax=ax, font_size=8, \n",
    "                           font_weight='bold')\n",
    "    \n",
    "    # Title\n",
    "    n_nodes = len(G.nodes())\n",
    "    n_paths = len(root_paths)\n",
    "    ax.set_title(f'Lineage Tree: {root_id}\\n{n_nodes} cells, {n_paths} complete paths',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   Saved network diagram to {out_file.name}\")\n",
    "\n",
    "print(\"âœ… Function loaded: plot_lineage_path_details(), plot_lineage_tree_network()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f5793f-ec7b-45ad-947a-58cad4216280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] Processing: 260116_A11_s5_spots.csv\n",
      "   Growth model proof added for 260116_A11_s5_spots\n",
      "   Diagnostic report saved to D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\260116_A11_s5_spots_CV_Report.txt\n",
      "   Saved lineage tree gallery to 260116_A11_s5_spots_Lineage_Trees.png\n",
      "   Saved 10 detailed path plots to Lineage_Path_Details/\n",
      "   Saved network diagram to 260116_A11_s5_spots_Network_61_0.png\n",
      "[2/4] Processing: 260116_A11_spoT_s6_spots.csv\n",
      "   â±ï¸  Avg: 3.8s/file | Est. remaining: 0.1 min\n",
      "   Growth model proof added for 260116_A11_spoT_s6_spots\n",
      "   Diagnostic report saved to D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\260116_A11_spoT_s6_spots_CV_Report.txt\n",
      "   Saved lineage tree gallery to 260116_A11_spoT_s6_spots_Lineage_Trees.png\n",
      "   Saved 10 detailed path plots to Lineage_Path_Details/\n",
      "   Saved network diagram to 260116_A11_spoT_s6_spots_Network_37_0.png\n",
      "[3/4] Processing: 260116_A11_spoT_s7_spots.csv\n",
      "   â±ï¸  Avg: 5.2s/file | Est. remaining: 0.1 min\n",
      "   Growth model proof added for 260116_A11_spoT_s7_spots\n",
      "   Diagnostic report saved to D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\260116_A11_spoT_s7_spots_CV_Report.txt\n",
      "   Saved lineage tree gallery to 260116_A11_spoT_s7_spots_Lineage_Trees.png\n",
      "   Saved 10 detailed path plots to Lineage_Path_Details/\n",
      "   Saved network diagram to 260116_A11_spoT_s7_spots_Network_113_0.png\n",
      "[4/4] Processing: 260116_A11_spoT_s8_spots.csv\n",
      "   â±ï¸  Avg: 6.0s/file | Est. remaining: 0.0 min\n",
      "   Growth model proof added for 260116_A11_spoT_s8_spots\n",
      "   Diagnostic report saved to D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\260116_A11_spoT_s8_spots_CV_Report.txt\n",
      "   Saved lineage tree gallery to 260116_A11_spoT_s8_spots_Lineage_Trees.png\n",
      "   Saved 10 detailed path plots to Lineage_Path_Details/\n",
      "   Saved network diagram to 260116_A11_spoT_s8_spots_Network_92_0.png\n",
      "\n",
      "âœ… Saved combined stats to D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\All_Branches_Stats.csv\n",
      "   Diagnostic report saved to D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\Combined_CV_Report.txt\n",
      "âœ… Generated Variance Diagnostic Report at D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\Combined_CV_Report.txt\n",
      "âœ… Saved lineage paths to D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\All_Lineage_Paths.csv\n",
      "\n",
      "============================================================\n",
      "LINEAGE PATH STATISTICS\n",
      "============================================================\n",
      "\n",
      "Path distribution:\n",
      "Path_Length  Quality_Tier\n",
      "1            No_Division     104\n",
      "2            Bronze          251\n",
      "             Silver          433\n",
      "3            Bronze           93\n",
      "             Gold             64\n",
      "             Silver          130\n",
      "4            Bronze          110\n",
      "             Gold             10\n",
      "             Silver           36\n",
      "5            Bronze           54\n",
      "             Gold              6\n",
      "             Silver           16\n",
      "\n",
      "\n",
      "Division timing (Gold tier only):\n",
      "  Division 1: 76.6 Â± 19.9 min (n=80)\n",
      "  Division 2: 19.6 Â± 12.1 min (n=80)\n",
      "  Division 3: 11.7 Â± 7.3 min (n=80)\n",
      "\n",
      "\n",
      "Non-dividing cells:\n",
      "  Total: 104 lineages never divided\n",
      "\n",
      "============================================================\n",
      "\n",
      "âœ… Complete! Total time: 0.6 minutes\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ MAIN EXECUTION LOOP\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Uses INPUT_FOLDER selected in Cell 4\n",
    "if INPUT_FOLDER:\n",
    "    # Find valid pairs of spot and edge files\n",
    "    spots_files = sorted(list(INPUT_FOLDER.glob('*spots.csv')))\n",
    "    edges_files = sorted(list(INPUT_FOLDER.glob('*edges.csv')))\n",
    "\n",
    "    valid_pairs = []\n",
    "    for s_path in spots_files:\n",
    "        # Construct expected edge file path\n",
    "        e_name = s_path.name.replace('_spots.csv', '_edges.csv')\n",
    "        e_path = s_path.parent / e_name\n",
    "\n",
    "        if e_path.exists():\n",
    "            valid_pairs.append((s_path, e_path))\n",
    "        else:\n",
    "            print(f\"âš ï¸  Missing corresponding edge file for {s_path.name}. Skipping.\")\n",
    "\n",
    "    if valid_pairs:\n",
    "        # Setup Output Directory (User-Friendly)\n",
    "        OUT_DIR = INPUT_FOLDER / \"Lineage_Path_Results\"\n",
    "        OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "        # [NEW] Setup Growth Model Proof Report\n",
    "        proof_path = OUT_DIR / \"Growth_Model_Proof.txt\"\n",
    "        with open(proof_path, \"w\") as f:\n",
    "            f.write(\"GROWTH MODEL PROOF REPORT\\n=========================\\n\\n\")\n",
    "\n",
    "        tracker = ProgressTracker(len(valid_pairs))\n",
    "\n",
    "        all_stats = []\n",
    "        all_lineage_paths = []\n",
    "\n",
    "        # BATCH PROCESS\n",
    "        for s_path, e_path in valid_pairs:\n",
    "            tracker.update(s_path.name)\n",
    "\n",
    "            try:\n",
    "                # 1. Load\n",
    "                spots, edges = load_trackmate_data(s_path, e_path)\n",
    "\n",
    "                # 2. Segment (with ROOT_ID)\n",
    "                annotated = segment_lineages(spots, edges)\n",
    "                if annotated.empty:\n",
    "                    print(\"   âš ï¸  No valid lineages found.\")\n",
    "                    continue\n",
    "\n",
    "                # 3. Robust Analysis (Stats)\n",
    "                file_stats = analyze_robust(annotated)\n",
    "                file_stats['Source_File'] = s_path.stem\n",
    "\n",
    "                # Metadata parsing\n",
    "                meta = parse_filename_metadata(s_path.stem)\n",
    "                file_stats['Gene'] = meta['Gene']\n",
    "                file_stats['Condition'] = meta['Condition']\n",
    "\n",
    "                all_stats.append(file_stats)\n",
    "\n",
    "                # 4. [NEW] Run Proof (Adder/Sizer/Timer)\n",
    "                prove_growth_model(file_stats, s_path.stem, proof_path)\n",
    "\n",
    "                # 5. [SURGICAL UPDATE] Per-File CV Report\n",
    "                calculate_cv_statistics(file_stats, OUT_DIR / f\"{s_path.stem}_CV_Report.txt\")\n",
    "\n",
    "                # 6. Extract Lineage Paths\n",
    "                lineage_paths = extract_lineage_paths(file_stats)\n",
    "                lineage_paths['Source_File'] = s_path.stem\n",
    "                lineage_paths.to_csv(OUT_DIR / f\"{s_path.stem}_Lineage_Paths.csv\", index=False)\n",
    "                all_lineage_paths.append(lineage_paths)\n",
    "\n",
    "                # 7. Plot Dashboard\n",
    "                plot_robust_dashboard(file_stats, annotated, OUT_DIR / f\"{s_path.stem}_Dashboard.png\")\n",
    "\n",
    "                # 8. Visualize Lineage Trees (6 examples)\n",
    "                plot_lineage_tree_gallery(lineage_paths, file_stats, annotated,\n",
    "                                         OUT_DIR / f\"{s_path.stem}_Lineage_Trees.png\",\n",
    "                                         n_examples=6)\n",
    "\n",
    "                # 9. Detailed path plots (top 10)\n",
    "                plot_lineage_path_details(lineage_paths, file_stats, annotated,\n",
    "                                         OUT_DIR, max_paths=10)\n",
    "\n",
    "                # 10. Network diagram\n",
    "                if not lineage_paths.empty:\n",
    "                    best_root = lineage_paths.sort_values('Path_Length', ascending=False).iloc[0]['Root_ID']\n",
    "                    plot_lineage_tree_network(lineage_paths, file_stats,\n",
    "                                             OUT_DIR / f\"{s_path.stem}_Network_{best_root}.png\",\n",
    "                                             root_id=best_root)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error processing {s_path.name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # FINAL AGGREGATION\n",
    "        # -------------------------------------------------------------\n",
    "        if all_stats:\n",
    "            final = pd.concat(all_stats, ignore_index=True)\n",
    "            final.to_csv(OUT_DIR / \"All_Branches_Stats.csv\", index=False)\n",
    "\n",
    "            # [SURGICAL UPDATE] Combined CV Report\n",
    "            print(f\"\\nâœ… Saved combined stats to {OUT_DIR / 'All_Branches_Stats.csv'}\")\n",
    "            calculate_cv_statistics(final, OUT_DIR / \"Combined_CV_Report.txt\")\n",
    "            print(f\"âœ… Generated Variance Diagnostic Report at {OUT_DIR / 'Combined_CV_Report.txt'}\")\n",
    "\n",
    "        if all_lineage_paths:\n",
    "            final_paths = pd.concat(all_lineage_paths, ignore_index=True)\n",
    "            final_paths.to_csv(OUT_DIR / \"All_Lineage_Paths.csv\", index=False)\n",
    "            print(f\"âœ… Saved lineage paths to {OUT_DIR / 'All_Lineage_Paths.csv'}\")\n",
    "\n",
    "            # Summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"LINEAGE PATH STATISTICS\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            print(\"\\nPath distribution:\")\n",
    "            print(final_paths.groupby(['Path_Length', 'Quality_Tier']).size().to_string())\n",
    "\n",
    "            print(\"\\n\\nDivision timing (Gold tier only):\")\n",
    "            gold = final_paths[final_paths['Quality_Tier'] == 'Gold']\n",
    "            if not gold.empty:\n",
    "                for i in [1, 2, 3]:\n",
    "                    # Prefer 't{i}_min' if available (from new code), else fallback to 't{i}'\n",
    "                    col_min = f't{i}_min'\n",
    "                    col_frame = f't{i}'\n",
    "\n",
    "                    if col_min in gold.columns and gold[col_min].notna().sum() > 0:\n",
    "                        mean_t = gold[col_min].mean()\n",
    "                        std_t = gold[col_min].std()\n",
    "                        n = gold[col_min].notna().sum()\n",
    "                        print(f\"  Division {i}: {mean_t:.1f} \\u00B1 {std_t:.1f} min (n={n})\")\n",
    "                    elif col_frame in gold.columns and gold[col_frame].notna().sum() > 0:\n",
    "                        mean_t = gold[col_frame].mean()\n",
    "                        std_t = gold[col_frame].std()\n",
    "                        n = gold[col_frame].notna().sum()\n",
    "                        print(f\"  Division {i}: {mean_t:.1f} \\u00B1 {std_t:.1f} frames (n={n})\")\n",
    "\n",
    "            print(\"\\n\\nNon-dividing cells:\")\n",
    "            no_div = final_paths[final_paths['Quality_Tier'] == 'No_Division']\n",
    "            print(f\"  Total: {len(no_div)} lineages never divided\")\n",
    "\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "        tracker.complete()\n",
    "    else:\n",
    "        print(\"âš ï¸  Skipped: No valid file pairs found in the input folder.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  Skipped: No input folder selected yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad539942-af8e-44bb-b77b-b9a1ad7e1dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PIPELINE STEP 2: POPULATION COMPARISON ---\n",
      "Select folder containing 'All_Branches_Stats.csv'...\n",
      "Scanning 6 CSV files...\n",
      "  Loaded: All_Branches_Stats.csv -> s5 A11\n",
      "\n",
      "Analyzing 2086 cells across 2 conditions...\n",
      "    -> Plotting Boss Three-Point Geometry...\n",
      "    -> Plotting Division Timing...\n",
      "âœ… Comparisons complete! Results saved to: D:\\Microscopy\\260116\\Omnipose Trackmate Output\\Lineage_Path_Results\\Population_Comparisons_Final\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Š PIPELINE STEP 2: MULTI-CONDITION POPULATION COMPARISON\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#\n",
    "# INPUT: 'All_Branches_Stats.csv' or multiple CSVs from Step 1.\n",
    "# OUTPUT: High-level comparison plots (Violin, Box, Scatter)\n",
    "#         Statistical tests (Mann-Whitney U, Cliff's Delta)\n",
    "#         Lineage analysis (Sister Asymmetry, Mother-Daughter Memory)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "# import tkinter as tk\n",
    "# from tkinter import filedialog # Commented out tkinter\n",
    "\n",
    "# Re-import for Colab-specific folder browsing if needed\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MIN_GENERATION = 1\n",
    "MAX_GENERATION = 3\n",
    "\n",
    "# Aesthetic Color Palette (Nature-inspired)\n",
    "EARTH_TONES = [\n",
    "    \"#264653\", # Charcoal/Teal\n",
    "    \"#2A9D8F\", # Sage Green\n",
    "    \"#E9C46A\", # Dark Gold / Maize\n",
    "    \"#F4A261\", # Sandy Orange\n",
    "    \"#E76F51\", # Terracotta\n",
    "    \"#6D597A\", # Dusty Purple\n",
    "    \"#B56576\", # Rose\n",
    "    \"#5F0F40\"  # Deep Burgundy\n",
    "]\n",
    "\n",
    "sns.set_palette(EARTH_TONES)\n",
    "\n",
    "# â”€â”€ 1. CONDITION DETECTION LOGIC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_condition(filename):\n",
    "    \"\"\"\n",
    "    Auto-detects biological condition from filename.\n",
    "    Prioritizes specific gene names, then drug treatments, then WT.\n",
    "    \"\"\"\n",
    "    name_lower = filename.lower()\n",
    "\n",
    "    # Gene Map: Maps filename substring -> Display Label\n",
    "    shape_genes = {\n",
    "        'mreb': 'MreB Mutant', 'mrec': 'MreC Mutant', 'mred': 'MreD Mutant',\n",
    "        'roda': 'RodA Mutant', 'rodz': 'RodZ Mutant', 'mrda': 'MrdA (PBP2) Mutant',\n",
    "        'pbp2': 'PBP2 Mutant', 'ftsz': 'FtsZ Mutant',\n",
    "        'minc': 'MinC Mutant', 'mind': 'MinD Mutant', 'mine': 'MinE Mutant',\n",
    "        'spot': 'spoT Mutant', 'nadr': 'NadR Mutant', 'topa': 'TopA Mutant',\n",
    "        'rsfs': 'RsfS Mutant'\n",
    "    }\n",
    "\n",
    "    # Priority 1: Specific Mutants\n",
    "    for gene, label in shape_genes.items():\n",
    "        if gene.lower() in name_lower:\n",
    "            return label\n",
    "\n",
    "    # Priority 2: Drug/Treatment\n",
    "    if 'drug' in name_lower or 'treatment' in name_lower:\n",
    "        return \"Drug Treatment\"\n",
    "\n",
    "    # Priority 3: Wild Type / Control\n",
    "    if any(x in name_lower for x in ['wt', 'wild_type', 'wildtype', 'control']):\n",
    "        return \"Wild Type\"\n",
    "\n",
    "    # Priority 4: Fallback\n",
    "    if 'mutant' in name_lower or 'mut' in name_lower:\n",
    "        return \"Mutant\"\n",
    "\n",
    "    return f\"Unknown ({filename[:15]}...)\"\n",
    "\n",
    "# â”€â”€ 2. HELPER: STATS & OUTLIER REMOVAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def remove_outliers(df, column):\n",
    "    \"\"\"Removes statistical outliers using the IQR method (1.5 * IQR).\"\"\"\n",
    "    if df.empty or column not in df.columns: return df\n",
    "    valid = df.dropna(subset=[column])\n",
    "    Q1 = valid[column].quantile(0.25)\n",
    "    Q3 = valid[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return valid[(valid[column] >= lower) & (valid[column] <= upper)]\n",
    "\n",
    "def cliffs_delta(x, y):\n",
    "    \"\"\"\n",
    "    Calculates Cliff's Delta (d) effect size.\n",
    "    Returns magnitude: 0.0 (overlap) to 1.0 (distinct).\n",
    "    Thresholds: Small > 0.147, Medium > 0.33, Large > 0.474\n",
    "    \"\"\"\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    x = np.asarray(x).flatten()\n",
    "    y = np.asarray(y).flatten()\n",
    "    nx, ny = len(x), len(y)\n",
    "\n",
    "    if nx == 0 or ny == 0: return 0.0\n",
    "\n",
    "    try:\n",
    "        u_stat, _ = mannwhitneyu(x, y, alternative='two-sided')\n",
    "        d = (2 * u_stat / (nx * ny)) - 1\n",
    "        return abs(d) # Return absolute magnitude\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fdr_correction(p_values, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Benjamini-Hochberg FDR correction for multiple comparisons.\n",
    "    Returns boolean array (True = Significant).\n",
    "    \"\"\"\n",
    "    p_values = np.asarray(p_values)\n",
    "    if len(p_values) == 0: return []\n",
    "\n",
    "    n = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_p = p_values[sorted_indices]\n",
    "\n",
    "    # Rank 1..n\n",
    "    ranks = np.arange(1, n + 1)\n",
    "\n",
    "    # Critical values = (rank/n) * alpha\n",
    "    critical_vals = (ranks / n) * alpha\n",
    "\n",
    "    # Find largest k where p_k <= critical_value\n",
    "    below = sorted_p <= critical_vals\n",
    "    if not np.any(below):\n",
    "        return np.zeros(n, dtype=bool)\n",
    "\n",
    "    k_max_idx = np.max(np.where(below)[0])\n",
    "\n",
    "    reject_sorted = np.zeros(n, dtype=bool)\n",
    "    reject_sorted[:k_max_idx+1] = True\n",
    "\n",
    "    reject = np.zeros(n, dtype=bool)\n",
    "    reject[sorted_indices] = reject_sorted\n",
    "    return reject\n",
    "\n",
    "def get_effect_star(is_significant, d):\n",
    "    \"\"\"\n",
    "    Returns star notation based on significance AND effect size.\n",
    "    Ensures stars are only shown if the effect is biologically relevant.\n",
    "    \"\"\"\n",
    "    if not is_significant: return \"\"\n",
    "    if d < 0.147: return \"\"      # Negligible effect\n",
    "\n",
    "    if d >= 0.474: return \"****\" # Large\n",
    "    if d >= 0.33:  return \"***\"  # Medium\n",
    "    if d >= 0.147: return \"*\"    # Small\n",
    "    return \"\"\n",
    "\n",
    "def run_stats_and_annotate(ax, data, x_col, y_col, hue_col, order, hue_order):\n",
    "    \"\"\"\n",
    "    Orchestrates the statistical pipeline:\n",
    "    1. Identify Wild Type control\n",
    "    2. Pairwise comparisons (Mutant vs WT)\n",
    "    3. FDR correction\n",
    "    4. Annotate plot with stars\n",
    "    \"\"\"\n",
    "    # 1. FIND CONTROL\n",
    "    cond_col = None\n",
    "    wt_name = None\n",
    "\n",
    "    search_cols = [hue_col, x_col] if hue_col else [x_col]\n",
    "    for col in search_cols:\n",
    "        if col not in data.columns: continue\n",
    "        if data[col].dtype == object or pd.api.types.is_string_dtype(data[col]):\n",
    "            unique_vals = data[col].astype(str).unique()\n",
    "            potential_wt = next((c for c in unique_vals if \"wild\" in c.lower() or \"wt\" in c.lower() or \"control\" in c.lower()), None)\n",
    "            if potential_wt:\n",
    "                cond_col = col\n",
    "                wt_name = potential_wt\n",
    "                break\n",
    "\n",
    "    if not wt_name: return # No control found, skip stats\n",
    "\n",
    "    # 2. COLLECT COMPARISONS\n",
    "    comparisons = []\n",
    "\n",
    "    # CASE A: X-axis is Condition (Boss Plot)\n",
    "    if cond_col == x_col:\n",
    "        wt_group = data[data[x_col] == wt_name]\n",
    "\n",
    "        for x_cat in order:\n",
    "            if str(x_cat) == str(wt_name): continue\n",
    "\n",
    "            current_hues = hue_order if hue_col else [None]\n",
    "\n",
    "            for hue_val in current_hues:\n",
    "                mask_mut = (data[x_col] == x_cat)\n",
    "                if hue_val is not None: mask_mut &= (data[hue_col] == hue_val)\n",
    "                mut_vals = data.loc[mask_mut, y_col].dropna()\n",
    "\n",
    "                mask_wt = pd.Series(True, index=wt_group.index)\n",
    "                if hue_val is not None: mask_wt &= (wt_group[hue_col] == hue_val)\n",
    "                wt_vals = wt_group.loc[mask_wt, y_col].dropna()\n",
    "\n",
    "                if len(mut_vals) > 3 and len(wt_vals) > 3:\n",
    "                    comparisons.append({\n",
    "                        'x': x_cat, 'hue': hue_val,\n",
    "                        'mut': mut_vals.values, 'wt': wt_vals.values\n",
    "                    })\n",
    "\n",
    "    # CASE B: Hue is Condition (Geometry Plot)\n",
    "    else:\n",
    "        for x_cat in order:\n",
    "            subset = data[data[x_col] == x_cat]\n",
    "            wt_vals = subset[subset[hue_col] == wt_name][y_col].dropna()\n",
    "\n",
    "            if len(wt_vals) < 3: continue\n",
    "\n",
    "            for hue_val in hue_order:\n",
    "                if str(hue_val) == str(wt_name): continue\n",
    "                mut_vals = subset[subset[hue_col] == hue_val][y_col].dropna()\n",
    "\n",
    "                if len(mut_vals) > 3:\n",
    "                    comparisons.append({\n",
    "                        'x': x_cat, 'hue': hue_val,\n",
    "                        'mut': mut_vals.values, 'wt': wt_vals.values\n",
    "                    })\n",
    "\n",
    "    if not comparisons: return\n",
    "\n",
    "    # 3. RUN TESTS\n",
    "    p_values = []\n",
    "    deltas = []\n",
    "\n",
    "    for comp in comparisons:\n",
    "        try:\n",
    "            _, p = stats.mannwhitneyu(comp['mut'], comp['wt'], alternative='two-sided')\n",
    "            d = cliffs_delta(comp['mut'], comp['wt'])\n",
    "            p_values.append(p)\n",
    "            deltas.append(d)\n",
    "        except:\n",
    "            p_values.append(1.0)\n",
    "            deltas.append(0.0)\n",
    "\n",
    "    is_significant = fdr_correction(p_values, alpha=0.05)\n",
    "\n",
    "    # 4. ANNOTATE STARS\n",
    "    n_hues = len(hue_order) if hue_order else 1\n",
    "    width = 0.8\n",
    "    if hue_order and n_hues > 1:\n",
    "        step = width / n_hues\n",
    "        offsets = np.linspace(-width/2 + step/2, width/2 - step/2, n_hues)\n",
    "        hue_offset_map = dict(zip(hue_order, offsets))\n",
    "    else:\n",
    "        hue_offset_map = {None: 0}\n",
    "\n",
    "    y_range = data[y_col].max() - data[y_col].min()\n",
    "\n",
    "    for i, comp in enumerate(comparisons):\n",
    "        star = get_effect_star(is_significant[i], deltas[i])\n",
    "\n",
    "        if star:\n",
    "            x_base = order.index(comp['x'])\n",
    "            x_shift = hue_offset_map.get(comp['hue'], 0)\n",
    "            x_pos = x_base + x_shift\n",
    "\n",
    "            y_pos = np.max(comp['mut']) + (y_range * 0.05)\n",
    "            color = '#D62828' if len(star) == 4 else 'black'\n",
    "\n",
    "            ax.text(x_pos, y_pos, star, ha='center', va='bottom',\n",
    "                    fontsize=8, fontweight='bold', color=color)\n",
    "\n",
    "\n",
    "# â”€â”€ 3. PLOTTING FUNCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def get_sorted_order(df, col='Condition'):\n",
    "    \"\"\"Returns sorted conditions with Wild Type always first.\"\"\"\n",
    "    if df.empty or col not in df.columns: return []\n",
    "    valid_data = df[col].dropna()\n",
    "    unique_conds = sorted(valid_data.astype(str).unique())\n",
    "\n",
    "    wt_match = next((x for x in unique_conds if x.lower() in ['wild type', 'wild_type', 'wt']), None)\n",
    "    if wt_match:\n",
    "        unique_conds.remove(wt_match)\n",
    "        unique_conds.insert(0, wt_match)\n",
    "\n",
    "    return unique_conds\n",
    "\n",
    "def plot_lineage_memory(df, out_path):\n",
    "    \"\"\"\n",
    "    Correlates Mother vs Daughter Duration.\n",
    "    Filter: Clean Divisions Only.\n",
    "    \"\"\"\n",
    "    merged = pd.merge(df, df, left_on='Cell_ID', right_on='Parent_ID', suffixes=('_M', '_D'))\n",
    "\n",
    "    if 'Is_Clean_Division_M' in merged.columns and 'Is_Clean_Division_D' in merged.columns:\n",
    "        clean = merged[(merged['Is_Clean_Division_M'] == True) &\n",
    "                       (merged['Is_Clean_Division_D'] == True)].copy()\n",
    "    else:\n",
    "        clean = merged.copy()\n",
    "\n",
    "    clean = remove_outliers(clean, 'Duration_M')\n",
    "    clean = remove_outliers(clean, 'Duration_D')\n",
    "    clean['Condition'] = clean['Condition_M']\n",
    "\n",
    "    if clean.empty:\n",
    "        print(\"   âš ï¸ Not enough mother-daughter pairs for memory plot.\")\n",
    "        return\n",
    "\n",
    "    g = sns.lmplot(data=clean, x='Duration_M', y='Duration_D', hue='Condition',\n",
    "                   col='Condition', height=5, aspect=1,\n",
    "                   scatter_kws={'alpha':0.5, 's':20}, line_kws={'lw':2})\n",
    "\n",
    "    for ax, (cond, subset) in zip(g.axes.flat, clean.groupby('Condition')):\n",
    "        if len(subset) > 5:\n",
    "            r, p = stats.pearsonr(subset['Duration_M'], subset['Duration_D'])\n",
    "            ax.text(0.05, 0.9, f'r={r:.2f}\\np={p:.3f}\\nN={len(subset)}', transform=ax.transAxes,\n",
    "                   fontsize=12, fontweight='bold', bbox=dict(facecolor='white', alpha=0.8))\n",
    "        ax.set_title(f\"{cond}\")\n",
    "\n",
    "    plt.suptitle(\"LINEAGE MEMORY: Heritability of Division Time\", y=1.05, fontsize=14, fontweight='bold')\n",
    "    plt.savefig(out_path / \"1_Lineage_Memory.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_sister_asymmetry(df, out_path):\n",
    "    \"\"\"\n",
    "    Compare sisters (A vs B) from the same mother.\n",
    "    \"\"\"\n",
    "    daughters = df[df['Parent_ID'].notna()]\n",
    "    sisters = pd.merge(daughters, daughters, on='Parent_ID', suffixes=('_A', '_B'))\n",
    "    sisters = sisters[sisters['Cell_ID_A'] < sisters['Cell_ID_B']].copy()\n",
    "\n",
    "    df['Unique_ID'] = df['Source_File'].astype(str) + \"_\" + df['Cell_ID'].astype(str)\n",
    "    parent_clean_lookup = df.set_index('Unique_ID')['Is_Clean_Division'].to_dict()\n",
    "\n",
    "    sisters['Unique_Parent_ID'] = sisters['Source_File_A'].astype(str) + \"_\" + sisters['Parent_ID'].astype(str)\n",
    "    sisters['Parent_Is_Clean'] = sisters['Unique_Parent_ID'].map(parent_clean_lookup)\n",
    "\n",
    "    clean_sisters = sisters[sisters['Parent_Is_Clean'] == True].copy()\n",
    "    if clean_sisters.empty: return\n",
    "\n",
    "    clean_sisters['Condition'] = clean_sisters['Condition_A']\n",
    "    clean_sisters['Dur_Diff'] = abs(clean_sisters['Duration_A'] - clean_sisters['Duration_B'])\n",
    "    clean_sisters = remove_outliers(clean_sisters, 'Dur_Diff')\n",
    "    cond_order = get_sorted_order(clean_sisters)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    ax = sns.ecdfplot(data=clean_sisters, x='Dur_Diff', hue='Condition',\n",
    "                      hue_order=cond_order, linewidth=2, palette=EARTH_TONES)\n",
    "\n",
    "    plt.title(\"SISTER ASYMMETRY: Synchrony of Division\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Absolute Difference in Division Time (Frames)\")\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1.05, 1), title='Condition')\n",
    "    plt.subplots_adjust(right=0.75, bottom=0.15)\n",
    "    plt.savefig(out_path / \"2_Sister_Asymmetry_Duration.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_size_control(df, out_path):\n",
    "    \"\"\"\n",
    "    Adder vs Timer vs Sizer Analysis (Slope of Added vs Birth Size).\n",
    "    \"\"\"\n",
    "    req_cols = ['Start_Area', 'End_Area']\n",
    "    if not all(c in df.columns for c in req_cols): return\n",
    "\n",
    "    df['Added_Area'] = df['End_Area'] - df['Start_Area']\n",
    "    valid = df[df['Added_Area'] > 0].copy()\n",
    "\n",
    "    if 'Is_Clean_Division' in valid.columns:\n",
    "        clean = valid[valid['Is_Clean_Division'] == True].copy()\n",
    "    else:\n",
    "        clean = valid.copy()\n",
    "\n",
    "    clean = remove_outliers(clean, 'Start_Area')\n",
    "    clean = remove_outliers(clean, 'Added_Area')\n",
    "\n",
    "    if clean.empty: return\n",
    "\n",
    "    g = sns.lmplot(data=clean, x='Start_Area', y='Added_Area', hue='Condition',\n",
    "                   col='Condition', height=5, aspect=1,\n",
    "                   scatter_kws={'alpha':0.4, 's':20}, line_kws={'lw':2, 'color':'black'})\n",
    "\n",
    "    for ax, (cond, subset) in zip(g.axes.flat, clean.groupby('Condition')):\n",
    "        if len(subset) > 5:\n",
    "            slope, _, r_val, _, _ = stats.linregress(subset['Start_Area'], subset['Added_Area'])\n",
    "            if -0.3 <= slope <= 0.3: mode = \"ADDER\"\n",
    "            elif slope > 0.7: mode = \"TIMER\"\n",
    "            elif slope < -0.7: mode = \"SIZER\"\n",
    "            else: mode = \"MIXED\"\n",
    "            ax.text(0.05, 0.85, f'Slope={slope:.2f}\\nRÂ²={r_val**2:.2f}\\n{mode}', transform=ax.transAxes,\n",
    "                   fontsize=11, fontweight='bold', bbox=dict(facecolor='white', alpha=0.8))\n",
    "        ax.set_title(f\"{cond}\")\n",
    "\n",
    "    plt.suptitle(\"SIZE CONTROL HOMEOSTASIS\", y=1.05, fontsize=14, fontweight='bold')\n",
    "    plt.savefig(out_path / \"3_Size_Control.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_geometry_comparisons(df, out_path):\n",
    "    \"\"\"\n",
    "    Generates comparison plots for all available geometric metrics.\n",
    "    \"\"\"\n",
    "    df_gen = df[(df['Generation'] >= MIN_GENERATION) &\n",
    "                (df['Generation'] <= MAX_GENERATION)].copy()\n",
    "\n",
    "    if 'Is_Clean_Division' in df_gen.columns:\n",
    "        df_gen = df_gen[df_gen['Is_Clean_Division'] == True]\n",
    "\n",
    "    if 'Mean_AR' not in df_gen.columns and 'Mean_Major' in df_gen.columns:\n",
    "        df_gen['Mean_AR'] = df_gen['Mean_Major'] / np.maximum(df_gen['Mean_Minor'], 1e-9)\n",
    "\n",
    "    metrics = [\n",
    "        ('Mean_Major', 'Length (Î¼m)', 'Cell Length'),\n",
    "        ('Mean_Minor', 'Width (Î¼m)', 'Cell Width'),\n",
    "        ('Mean_AR', 'Aspect Ratio', 'Aspect Ratio'),\n",
    "        ('Mean_Area', 'Area (pxÂ²)', 'Cell Area'),\n",
    "        ('Growth_Rate_k_min', 'Specific Rate k (minâ»Â¹)', 'Specific Growth Rate'),\n",
    "        ('Doubling_Time_min', 'Minutes', 'Doubling Time'),\n",
    "        ('Duration_Minutes', 'Minutes', 'Division Duration (Min)'),\n",
    "        ('Mean_Perimeter', 'Perimeter (Î¼m)', 'Cell Perimeter'),\n",
    "        ('Mean_Circularity', 'Circularity (0-1)', 'Cell Circularity'),\n",
    "        ('Mean_Solidity', 'Solidity (0-1)', 'Cell Solidity'),\n",
    "        ('Mean_Shape_Index', 'Shape Index', 'Shape Index'),\n",
    "        ('Mean_Angle', 'Angle (Radians)', 'Orientation Angle')\n",
    "    ]\n",
    "\n",
    "    cond_order = get_sorted_order(df_gen)\n",
    "\n",
    "    for i, (col, ylabel, title) in enumerate(metrics):\n",
    "        if col not in df_gen.columns: continue\n",
    "\n",
    "        clean_data = remove_outliers(df_gen, col)\n",
    "        if clean_data.empty: continue\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        ax = sns.stripplot(data=clean_data, x='Generation', y=col, hue='Condition',\n",
    "                      hue_order=cond_order, palette=EARTH_TONES,\n",
    "                      dodge=True, alpha=0.7, size=4, jitter=0.25, zorder=0)\n",
    "\n",
    "        sns.boxplot(data=clean_data, x='Generation', y=col, hue='Condition',\n",
    "                    hue_order=cond_order, palette=EARTH_TONES,\n",
    "                    showfliers=False, boxprops={'alpha': 0.6, 'edgecolor': 'black'}, zorder=10)\n",
    "\n",
    "        run_stats_and_annotate(ax, clean_data, 'Generation', col, 'Condition',\n",
    "                               sorted(clean_data['Generation'].unique()), cond_order)\n",
    "\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        if handles:\n",
    "            plt.legend(handles[:len(cond_order)], labels[:len(cond_order)], title='Condition',\n",
    "                       bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "        plt.title(f\"{title} (Clean Divisions Only)\", fontweight='bold')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.subplots_adjust(bottom=0.20, right=0.80)\n",
    "        plt.savefig(out_path / f\"4{chr(97+i)}_{title.replace(' ', '_')}.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "def plot_boss_three_point_geometry(df, out_path):\n",
    "    \"\"\"\n",
    "    Advanced 'Boss Plot': Tracks geometry across Birth -> Division -> Daughter.\n",
    "    \"\"\"\n",
    "    print(\"    -> Plotting Boss Three-Point Geometry...\")\n",
    "    base_features = set()\n",
    "    for col in df.columns:\n",
    "        if col.startswith('Start_') and f'End_{col[6:]}' in df.columns:\n",
    "            base_features.add(col[6:])\n",
    "\n",
    "    if not base_features: return\n",
    "\n",
    "    # Build linkage\n",
    "    start_values = df.set_index(['Source_File', 'Cell_ID']).to_dict(orient='index')\n",
    "\n",
    "    df_linked = df.copy()\n",
    "    # Create lookup for daughters\n",
    "    daughters_map = {}\n",
    "    for _, row in df[df['Parent_ID'].notna()].iterrows():\n",
    "        key = (row['Source_File'], row['Parent_ID'])\n",
    "        if key not in daughters_map: daughters_map[key] = []\n",
    "        daughters_map[key].append(row['Cell_ID'])\n",
    "\n",
    "    # Calculate linked daughter values\n",
    "    for feature in base_features:\n",
    "        col_linked = f'Linked_Daughter_Start_{feature}'\n",
    "        linked_vals = []\n",
    "        for _, row in df_linked.iterrows():\n",
    "            children = daughters_map.get((row['Source_File'], row['Cell_ID']), [])\n",
    "            vals = []\n",
    "            for child in children:\n",
    "                key = (row['Source_File'], child)\n",
    "                if key in start_values:\n",
    "                    val = start_values[key].get(f'Start_{feature}', np.nan)\n",
    "                    if pd.notna(val): vals.append(val)\n",
    "            linked_vals.append(np.median(vals) if vals else np.nan)\n",
    "        df_linked[col_linked] = linked_vals\n",
    "\n",
    "    df_clean = df_linked[df_linked['Is_Clean_Division'] == True].copy()\n",
    "    if df_clean.empty: return\n",
    "    cond_order = get_sorted_order(df)\n",
    "\n",
    "    for feature in sorted(base_features):\n",
    "        col_s, col_e = f'Start_{feature}', f'End_{feature}'\n",
    "        col_l = f'Linked_Daughter_Start_{feature}'\n",
    "\n",
    "        if df_clean[col_l].isna().all(): continue\n",
    "\n",
    "        melted = df_clean.melt(id_vars=['Condition'], value_vars=[col_s, col_e, col_l],\n",
    "                               var_name='Stage', value_name='Value')\n",
    "        melted['Stage'] = melted['Stage'].map({col_s:'1. Birth', col_e:'2. Division', col_l:'3. Daughter'})\n",
    "\n",
    "        for version in ['WithOutliers', 'NoOutliers']:\n",
    "            plot_data = melted.copy()\n",
    "            if version == 'NoOutliers':\n",
    "                plot_data = melted.groupby(['Condition', 'Stage']).apply(lambda x: remove_outliers(x, 'Value')).reset_index(drop=True)\n",
    "\n",
    "            if plot_data.empty: continue\n",
    "\n",
    "            plt.figure(figsize=(12, 9))\n",
    "            ax = sns.stripplot(data=plot_data, x='Condition', y='Value', hue='Stage',\n",
    "                          order=cond_order, palette=[\"#2A9D8F\", \"#E76F51\", \"#264653\"],\n",
    "                          dodge=True, jitter=True, size=3, alpha=0.4)\n",
    "            sns.boxplot(data=plot_data, x='Condition', y='Value', hue='Stage',\n",
    "                        order=cond_order, palette=[\"#2A9D8F\", \"#E76F51\", \"#264653\"],\n",
    "                        dodge=True, showfliers=False, width=0.8)\n",
    "\n",
    "            run_stats_and_annotate(ax, plot_data, 'Condition', 'Value', 'Stage', cond_order, ['1. Birth', '2. Division', '3. Daughter'])\n",
    "            plt.title(f\"Cell {feature} Cycle ({version})\", fontweight='bold')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.subplots_adjust(bottom=0.20)\n",
    "            plt.savefig(out_path / f\"ThreePoint_{feature}_{version}.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "def plot_boss_division_timing(df, out_path):\n",
    "    \"\"\"\n",
    "    Plots time to 1st, 2nd, 3rd division (With & Without Outliers).\n",
    "    \"\"\"\n",
    "    print(\"    -> Plotting Division Timing...\")\n",
    "    sub = df[df['Generation'].between(1, 3)].copy()\n",
    "    if 'Is_Clean_Division' in sub.columns:\n",
    "        sub = sub[sub['Is_Clean_Division'] == True]\n",
    "\n",
    "    if sub.empty: return\n",
    "    cond_order = get_sorted_order(df)\n",
    "\n",
    "    for version in ['WithOutliers', 'NoOutliers']:\n",
    "        plot_data = sub.copy()\n",
    "        if version == 'NoOutliers':\n",
    "            plot_data = sub.groupby(['Condition', 'Generation']).apply(lambda x: remove_outliers(x, 'Duration_Minutes')).reset_index(drop=True)\n",
    "\n",
    "        if plot_data.empty: continue\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        ax = sns.stripplot(data=plot_data, x='Condition', y='Duration_Minutes', hue='Generation',\n",
    "                      order=cond_order, palette='viridis', dodge=True, jitter=True, size=3, alpha=0.4)\n",
    "        sns.boxplot(data=plot_data, x='Condition', y='Duration_Minutes', hue='Generation',\n",
    "                    order=cond_order, palette='viridis', dodge=True, showfliers=False, width=0.8)\n",
    "\n",
    "        run_stats_and_annotate(ax, plot_data, 'Condition', 'Duration_Minutes', 'Generation', cond_order, [1, 2, 3])\n",
    "        plt.title(f\"Division Timing ({version})\", fontweight='bold')\n",
    "        plt.ylabel(\"Minutes\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.subplots_adjust(bottom=0.20)\n",
    "        plt.savefig(out_path / f\"Time_to_Division_{version}.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "def generate_boss_summary_table(df, out_path):\n",
    "    \"\"\"Summarizes key metrics per condition into a CSV.\"\"\"\n",
    "    stats_list = []\n",
    "    for cond in df['Condition'].unique():\n",
    "        sub = df[df['Condition'] == cond]\n",
    "        k_max = sub['Max_Instant_k_min'].median() if 'Max_Instant_k_min' in sub.columns else np.nan\n",
    "        t1 = sub[sub['Generation'] == 1]['Duration_Minutes'].median()\n",
    "        stats_list.append({\n",
    "            'Condition': cond, 'Max_Growth_Rate': round(k_max, 4),\n",
    "            'Time_Div_1': round(t1, 1), 'N_Cells': len(sub)\n",
    "        })\n",
    "    pd.DataFrame(stats_list).to_csv(out_path / \"Boss_Summary_Table.csv\", index=False)\n",
    "\n",
    "# Reuse the _browse_dirs_cli from earlier cells for Colab compatibility\n",
    "def _browse_dirs_cli(start_dir: Path) -> Path:\n",
    "    \"\"\"Simple interactive directory browser (works in Colab).\"\"\"\n",
    "    cur = start_dir\n",
    "\n",
    "    while True:\n",
    "        if not cur.exists():\n",
    "            print(f\"âŒ Folder not found: {cur}\")\n",
    "            cur = start_dir\n",
    "            continue\n",
    "\n",
    "        # Get subdirectories only\n",
    "        try:\n",
    "            subdirs = sorted([p for p in cur.iterdir() if p.is_dir()],\n",
    "                             key=lambda p: p.name.lower())\n",
    "        except PermissionError:\n",
    "            print(\"ğŸš« Permission denied. Going up...\")\n",
    "            cur = cur.parent\n",
    "            continue\n",
    "\n",
    "        print(\"\\n\" + \"â”€\" * 70)\n",
    "        print(f\"ğŸ“ Current folder: {cur}\")\n",
    "        print(\"Type:\")\n",
    "        print(\"  0  = âœ… SELECT THIS FOLDER\")\n",
    "        print(\"  .. = â¬†ï¸ Go up one level\")\n",
    "        print(\"\\nSubfolders:\")\n",
    "        for i, d in enumerate(subdirs, start=1):\n",
    "            print(f\"  {i:>2}. {d.name}/\")\n",
    "\n",
    "        choice = input(\"\\nğŸ‘‡ Select number (or '0' to finish): \").strip()\n",
    "\n",
    "        if choice == \"0\":\n",
    "            return cur\n",
    "\n",
    "        if choice == \"..\":\n",
    "            cur = cur.parent\n",
    "            continue\n",
    "\n",
    "        if choice.isdigit():\n",
    "            idx = int(choice)\n",
    "            if 1 <= idx <= len(subdirs):\n",
    "                cur = subdirs[idx - 1]\n",
    "                continue\n",
    "\n",
    "        # Handle manual path pasting or errors\n",
    "        p = Path(choice)\n",
    "        if p.is_absolute() and p.exists() and p.is_dir():\n",
    "            cur = p\n",
    "        else:\n",
    "            print(\"âŒ Invalid selection. Enter a number, '..', '0', or a valid absolute path.\")\n",
    "\n",
    "def select_output_folder_colab():\n",
    "    \"\"\"Colab-aware folder picker for output folder selection.\"\"\"\n",
    "    print(\"\\nğŸš€ Launching Folder Browser for Step 2 Input (All_Branches_Stats.csv location)... \")\n",
    "    # Start browser at MyDrive, or current working directory if not in Colab\n",
    "    start_path = Path(\"/content/drive/MyDrive\") if IN_COLAB else Path(\".\")\n",
    "    if not start_path.exists():\n",
    "        # Fallback to content if MyDrive isn't mounted\n",
    "        start_path = Path(\"/content\")\n",
    "        if not start_path.exists():\n",
    "            print(\"\\nâš ï¸  Could not find a suitable starting directory for browsing.\")\n",
    "            return None\n",
    "\n",
    "    return _browse_dirs_cli(start_path)\n",
    "\n",
    "# â”€â”€ 4. MAIN EXECUTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main_step2():\n",
    "    # root = tk.Tk(); root.withdraw() # Commented out tkinter GUI elements\n",
    "\n",
    "    print(\"\\n--- PIPELINE STEP 2: MULTI-CONDITION POPULATION COMPARISON ---\")\n",
    "    print(\"Select the folder containing 'All_Branches_Stats.csv' from the previous step.\")\n",
    "    \n",
    "    folder = None\n",
    "    if IN_COLAB:\n",
    "        folder = select_output_folder_colab()\n",
    "    else:\n",
    "        # Local Jupyter: use system dialog\n",
    "        import tkinter as tk\n",
    "        from tkinter import filedialog\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)\n",
    "\n",
    "        print(\"Please check your taskbar for the folder selection window...\")\n",
    "        folder = filedialog.askdirectory(title=\"Select Output Folder Lineage_Path_Results\")\n",
    "        root.destroy()\n",
    "\n",
    "    if not folder: \n",
    "        print(\"âŒ No folder selected. Exiting Step 2.\")\n",
    "        return\n",
    "    folder = Path(folder)\n",
    "\n",
    "    out_dir = folder / \"Population_Comparisons_Final\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Explicitly look for 'All_Branches_Stats.csv' in the selected folder\n",
    "    all_branches_stats_path = folder / \"All_Branches_Stats.csv\"\n",
    "    valid_dfs = []\n",
    "\n",
    "    if all_branches_stats_path.exists():\n",
    "        try:\n",
    "            full_df = pd.read_csv(all_branches_stats_path)\n",
    "            if 'Cell_ID' in full_df.columns:\n",
    "                # Ensure 'Condition' is populated\n",
    "                if 'Condition' not in full_df.columns or full_df['Condition'].isnull().all():\n",
    "                    print(\"Attempting to infer conditions from individual file names...\")\n",
    "                    # This part needs to be careful if All_Branches_Stats.csv was not generated with Source_File info\n",
    "                    # For now, assume previous step populated Source_File and Condition correctly\n",
    "                    if 'Source_File' in full_df.columns:\n",
    "                        full_df['Condition'] = full_df.apply(lambda row: get_condition(row['Source_File']) if pd.isna(row['Condition']) else row['Condition'], axis=1)\n",
    "                    else:\n",
    "                        print(\"âš ï¸  'Source_File' column not found, cannot infer conditions. Please ensure 'Condition' is present in 'All_Branches_Stats.csv'.\")\n",
    "                        return\n",
    "                valid_dfs.append(full_df)\n",
    "                print(f\"  Loaded: {all_branches_stats_path.name}\")\n",
    "            else:\n",
    "                print(f\"âŒ '{all_branches_stats_path.name}' does not contain expected 'Cell_ID' column. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading '{all_branches_stats_path.name}': {e}. Skipping.\")\n",
    "    else:\n",
    "        print(f\"âŒ '{all_branches_stats_path.name}' not found in the selected folder: {folder}. Please select the folder containing this file.\")\n",
    "\n",
    "    if not valid_dfs:\n",
    "        print(\"âŒ No valid stats files found for population comparison.\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(valid_dfs, ignore_index=True)\n",
    "    combined.to_csv(out_dir / \"Total_Population_Dataset.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nAnalyzing {len(combined)} cells across {combined['Condition'].nunique()} conditions...\")\n",
    "\n",
    "    try:\n",
    "        plot_lineage_memory(combined, out_dir)\n",
    "        plot_sister_asymmetry(combined, out_dir)\n",
    "        plot_size_control(combined, out_dir)\n",
    "        plot_geometry_comparisons(combined, out_dir)\n",
    "        plot_boss_three_point_geometry(combined, out_dir)\n",
    "        plot_boss_division_timing(combined, out_dir)\n",
    "        generate_boss_summary_table(combined, out_dir)\n",
    "        print(f\"âœ… Comparisons complete! Results saved to: {out_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during plotting: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure Google Drive is mounted if in Colab and not already mounted.\n",
    "    if IN_COLAB and not Path(\"/content/drive\").exists():\n",
    "        print(\"ğŸ“Œ Mounting Google Drive...\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "    main_step2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9aaf68-7f94-4dc4-b4dc-1aa520f42b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Microscopy",
   "language": "python",
   "name": "micrscopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
